{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "opt_jax.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/probml/pyprobml/blob/master/book1/optimization/opt_jax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b520E1nCIBHc"
      },
      "source": [
        "# Optimization (JAX version)\n",
        "\n",
        "In this notebook, we explore various  algorithms\n",
        "for solving optimization problems of the form\n",
        "$$\n",
        "x* = \\arg \\min_{x \\in X} f(x)\n",
        "$$\n",
        "We focus on the case where $f: R^D \\rightarrow R$ is a differentiable function\n",
        "\n",
        "## TOC\n",
        "* [Automatic differentiation](#AD)\n",
        "* [Second-order full-batch optimization](#second)\n",
        "* [Stochastic gradient descent](#SGD)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeuOgABaIENZ"
      },
      "source": [
        "import sklearn\n",
        "import scipy\n",
        "import scipy.optimize\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import itertools\n",
        "import time\n",
        "from functools import partial\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "#np.set_printoptions(precision=3)\n",
        "np.set_printoptions(formatter={'float': lambda x: \"{0:0.5f}\".format(x)})"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNQHpyKLIx_P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88ddc2a4-a496-4370-87c3-7733fd34c6eb"
      },
      "source": [
        "# https://github.com/google/jax\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "from jax.scipy.special import logsumexp\n",
        "from jax import grad, hessian, jacfwd, jacrev, jit, vmap\n",
        "from jax.experimental import optimizers\n",
        "print(\"jax version {}\".format(jax.__version__))\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jax version 0.2.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br921MsmKQkt"
      },
      "source": [
        "# Fit a binary logistic regression model using sklearn\n",
        "\n",
        "As a running example, we will use binary logistic regression on the iris dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3fX16J4IoL_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7f1ae99-4dbe-43d9-eef6-e3b7abf6217a"
      },
      "source": [
        "# Fit the model to a dataset, so we have an \"interesting\" parameter vector to use.\n",
        "\n",
        "import sklearn.datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "iris = sklearn.datasets.load_iris()\n",
        "X = iris[\"data\"]\n",
        "y = (iris[\"target\"] == 2).astype(np.int)  # 1 if Iris-Virginica, else 0'\n",
        "N, D = X.shape # 150, 4\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# We set C to a large number to turn off regularization.\n",
        "# We don't fit the bias term to simplify the comparison below.\n",
        "log_reg = LogisticRegression(solver=\"lbfgs\", C=1e5, fit_intercept=False)\n",
        "log_reg.fit(X_train, y_train)\n",
        "w_mle_sklearn = jnp.ravel(log_reg.coef_)\n",
        "print(w_mle_sklearn)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-4.414 -9.111  6.539 12.686]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_HQrexrySmT"
      },
      "source": [
        "# Objectives and their gradients\n",
        "\n",
        "The key input to an optimization algorithm (aka solver) is the objective function and its gradient. As an example, we use negative log likelihood for a binary logistic regression model as the objective. We compute the gradient by hand, and also use JAX's autodiff feature.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pIgD7iRLUBt"
      },
      "source": [
        "## Manual differentiation <a class=\"anchor\" id=\"AD\"></a>\n",
        "\n",
        "We compute the gradient of the negative log likelihood for binary logistic regression applied to the Iris dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS5AB9NjLZ_i"
      },
      "source": [
        "\n",
        "# Binary cross entropy\n",
        "def BCE_with_logits(logits, targets):\n",
        "  #BCE = -sum_n log(p1)*yn + log(p0)*y0\n",
        "  #p1 = 1/(1+exp(-a)\n",
        "  #log(p1) = log(1) - log(1+exp(-a)) = 0 - logsumexp(0, -a)\n",
        "  N = logits.shape[0]\n",
        "  logits = logits.reshape(N,1)\n",
        "  logits_plus = jnp.hstack([jnp.zeros((N,1)), logits]) # e^0=1\n",
        "  logits_minus = jnp.hstack([jnp.zeros((N,1)), -logits])\n",
        "  logp1 = -logsumexp(logits_minus, axis=1)\n",
        "  logp0 = -logsumexp(logits_plus, axis=1)\n",
        "  logprobs = logp1 * targets + logp0 * (1-targets)\n",
        "  return -jnp.sum(logprobs)/N\n",
        "\n",
        "def sigmoid(x): return 0.5 * (jnp.tanh(x / 2.) + 1)\n",
        "\n",
        "def predict_logit(weights, inputs):\n",
        "    return jnp.dot(inputs, weights) \n",
        "\n",
        "def predict_prob(weights, inputs):\n",
        "    return sigmoid(predict_logit(weights, inputs))\n",
        "\n",
        "def NLL(weights, batch):\n",
        "    X, y = batch\n",
        "    logits = predict_logit(weights, X)\n",
        "    return BCE_with_logits(logits, y)\n",
        "\n",
        "def NLL_grad(weights, batch):\n",
        "    X, y = batch\n",
        "    N = X.shape[0]\n",
        "    mu = predict_prob(weights, X)\n",
        "    g = jnp.sum(jnp.dot(jnp.diag(mu - y), X), axis=0)/N\n",
        "    return g\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0nT1ASb86iJ",
        "outputId": "ee95b841-a003-4750-e270-622c09473941"
      },
      "source": [
        "w = w_mle_sklearn\n",
        "y_pred = predict_prob(w, X_test)\n",
        "loss = NLL(w, (X_test, y_test))\n",
        "grad_np = NLL_grad(w, (X_test, y_test))\n",
        "print(\"params {}\".format(w))\n",
        "#print(\"pred {}\".format(y_pred))\n",
        "print(\"loss {}\".format(loss))\n",
        "print(\"grad {}\".format(grad_np))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "params [-4.414 -9.111  6.539 12.686]\n",
            "loss 0.11824002861976624\n",
            "grad [-0.235 -0.122 -0.198 -0.064]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLyk46HbLhgT"
      },
      "source": [
        "## Automatic differentiation in JAX  <a class=\"anchor\" id=\"AD-jax\"></a>\n",
        "\n",
        "Below we use JAX to compute the gradient of the NLL for binary logistic regression.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GkR1yHNLcjU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c6adb38-7892-4f12-8ccb-bbd3aea2518b"
      },
      "source": [
        "\n",
        "grad_jax = grad(NLL)(w, (X_test, y_test))\n",
        "print(\"grad {}\".format(grad_jax))\n",
        "assert np.allclose(grad_np, grad_jax)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "grad [-0.235 -0.122 -0.198 -0.064]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BXji_6BL87s"
      },
      "source": [
        "# Second-order, full-batch optimization <a class=\"anchor\" id=\"second\"></a>\n",
        "\n",
        "The \"gold standard\" of optimization is second-order methods, that leverage Hessian information. Since the Hessian has O(D^2) parameters, such methods do not scale to high-dimensional problems. However, we can sometimes approximate the Hessian using low-rank or diagonal approximations. Below we illustrate the low-rank BFGS method, and the limited-memory version of BFGS, that uses O(D H) space and O(D^2) time per step, where H is the history length.\n",
        "\n",
        "In general, second-order methods also require exact (rather than noisy) gradients. In the context of ML, this means they are \"full batch\" methods, since computing the exact gradient requires evaluating the loss on all the datapoints. However, for small data problems, this is feasible (and advisable).\n",
        "\n",
        "Below we illustrate how to use LBFGS as in [scipy.optimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize)\n",
        "                    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkTaK-WZMAGL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53d4d12b-a57e-48ee-8a28-d044f22242b3"
      },
      "source": [
        "import scipy.optimize\n",
        "\n",
        "def training_loss(w):\n",
        "    return NLL(w, (X_train, y_train))\n",
        "\n",
        "def training_grad(w):\n",
        "    return NLL_grad(w, (X_train, y_train))\n",
        "\n",
        "np.random.seed(42)\n",
        "w_init = np.random.randn(D)\n",
        "\n",
        "options={'disp': None,   'maxfun': 1000, 'maxiter': 1000}\n",
        "method = 'BFGS'\n",
        "# The gradient function is specified via the Jacobian keyword\n",
        "w_mle_scipy = scipy.optimize.minimize(training_loss, w_init, jac=training_grad, method=method, options=options).x   \n",
        "\n",
        "print(\"parameters from sklearn {}\".format(w_mle_sklearn))\n",
        "print(\"parameters from scipy-bfgs {}\".format(w_mle_scipy))\n",
        "assert np.allclose(w_mle_sklearn, w_mle_scipy, atol=1e-1)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "parameters from sklearn [-4.414 -9.111  6.539 12.686]\n",
            "parameters from scipy-bfgs [-4.415 -9.115  6.541 12.692]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5cLYkceMG7A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67bdd114-0442-483f-d298-af0715bbc5d2"
      },
      "source": [
        "# Limited memory version requires that we work with 64bit, since implemented in Fortran.\n",
        "\n",
        "def training_loss_64bit(w):\n",
        "    l = NLL(w, (X_train, y_train))\n",
        "    return np.float64(l)\n",
        "\n",
        "def training_grad_64bit(w):\n",
        "    g = NLL_grad(w, (X_train, y_train))\n",
        "    return np.asarray(g, dtype=np.float64)\n",
        "\n",
        "np.random.seed(42)\n",
        "w_init = np.random.randn(D)                 \n",
        "\n",
        "memory = 10\n",
        "options={'disp': None, 'maxcor': memory,  'maxfun': 1000, 'maxiter': 1000}\n",
        "# The code also handles bound constraints, hence the name\n",
        "method = 'L-BFGS-B'\n",
        "#w_mle_scipy = scipy.optimize.minimize(training_loss, w_init, jac=training_grad, method=method).x \n",
        "w_mle_scipy = scipy.optimize.minimize(training_loss_64bit, w_init, jac=training_grad_64bit, method=method).x \n",
        "\n",
        "\n",
        "print(\"parameters from sklearn {}\".format(w_mle_sklearn))\n",
        "print(\"parameters from scipy-lbfgs {}\".format(w_mle_scipy))\n",
        "assert np.allclose(w_mle_sklearn, w_mle_scipy, atol=1e-1)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "parameters from sklearn [-4.414 -9.111  6.539 12.686]\n",
            "parameters from scipy-lbfgs [-4.415 -9.114  6.54  12.692]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiZXds_DMj31"
      },
      "source": [
        "# Stochastic gradient descent <a class=\"anchor\" id=\"SGD\"></a>\n",
        "\n",
        "In this section we  illustrate how to implement SGD. We apply it to a simple convex problem, namely MLE for binary logistic regression on the small iris dataset, so we can compare to the exact batch methods we illustrated above.\n",
        "We make use of [Tensorflow datasets](https://github.com/probml/pyprobml/tree/master/book1/intro/tfds_intro.ipynb) to convert our data to a stream of minibatches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtOeheP-MnB7"
      },
      "source": [
        "## Numpy version\n",
        "We show a minimal implementation of SGD using vanilla numpy. For convenience, we use TFDS to create a stream of mini-batches. We compute gradients by hand, but can use any AD library.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fcr5EQg-3ix",
        "outputId": "20cbcb7c-cc92-42b8-b8f6-4d5c74647540"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "def make_batch_stream(X_train, y_train, batch_size):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices({\"X\": X_train, \"y\": y_train})\n",
        "  batches = dataset.batch(batch_size)\n",
        "  batch_stream = tfds.as_numpy(batches)  # finite iterable of dict of NumPy arrays\n",
        "  N = X_train.shape[0]\n",
        "  nbatches = int(np.floor(N/batch_size))\n",
        "  print('{} examples split into {} batches of size {}'.format(N, nbatches, batch_size))\n",
        "  return batch_stream\n",
        "\n",
        "batch_stream = make_batch_stream(X_train, y_train, 20)\n",
        "for epoch in range(2):\n",
        "  print('epoch {}'.format(epoch))\n",
        "  for batch in batch_stream:\n",
        "    x, y = batch[\"X\"], batch[\"y\"]\n",
        "    print(x.shape) # batch size * num features = 4"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100 examples split into 5 batches of size 20\n",
            "epoch 0\n",
            "(20, 4)\n",
            "(20, 4)\n",
            "(20, 4)\n",
            "(20, 4)\n",
            "(20, 4)\n",
            "epoch 1\n",
            "(20, 4)\n",
            "(20, 4)\n",
            "(20, 4)\n",
            "(20, 4)\n",
            "(20, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wG9tVufuMTui"
      },
      "source": [
        "def sgd(params, loss_fn, grad_loss_fn, batch_iter, max_epochs, lr):\n",
        "    print_every = max(1, int(0.1*max_epochs))\n",
        "    for epoch in range(max_epochs):\n",
        "        epoch_loss = 0.0\n",
        "        for batch_dict in batch_iter:\n",
        "            x, y = batch_dict[\"X\"], batch_dict[\"y\"]\n",
        "            batch = (x, y)\n",
        "            batch_grad = grad_loss_fn(params, batch)\n",
        "            params = params - lr*batch_grad\n",
        "            batch_loss = loss_fn(params, batch) # Average loss within this batch\n",
        "            epoch_loss += batch_loss\n",
        "        if epoch % print_every == 0:\n",
        "            print('Epoch {}, batch Loss {}'.format(epoch, batch_loss))\n",
        "    return params,\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sV3NbjvM6ai",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3721299-6730-4d49-9cf5-a6903e96e223"
      },
      "source": [
        "np.random.seed(42)\n",
        "w_init = np.random.randn(D) \n",
        "\n",
        "max_epochs = 5\n",
        "lr = 0.1\n",
        "batch_size = 10\n",
        "batch_stream = make_batch_stream(X_train, y_train, batch_size)\n",
        "w_mle_sgd = sgd(w_init, NLL, NLL_grad, batch_stream, max_epochs, lr)\n",
        "print(w_mle_sgd)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100 examples split into 10 batches of size 10\n",
            "Epoch 0, batch Loss 0.36490145325660706\n",
            "Epoch 1, batch Loss 0.34500643610954285\n",
            "Epoch 2, batch Loss 0.32851698994636536\n",
            "Epoch 3, batch Loss 0.3143332600593567\n",
            "Epoch 4, batch Loss 0.3018316626548767\n",
            "(DeviceArray([-0.538, -0.827,  0.613,  1.661], dtype=float32),)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtFGH_OeZUVj"
      },
      "source": [
        "## Jax version <a class=\"anchor\" id=\"SGD-jax\"></a>\n",
        "\n",
        "JAX has a small optimization library focused on stochastic first-order optimizers. Every optimizer is modeled as an (`init_fun`, `update_fun`, `get_params`) triple of functions. The `init_fun` is used to initialize the optimizer state, which could include things like momentum variables, and the `update_fun` accepts a gradient and an optimizer state to produce a new optimizer state. The `get_params` function extracts the current iterate (i.e. the current parameters) from the optimizer state. The parameters being optimized can be ndarrays or arbitrarily-nested list/tuple/dict structures, so you can store your parameters however youâ€™d like.\n",
        "\n",
        "Below we show how to reproduce our numpy code using this library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtBbjnzRM79T"
      },
      "source": [
        "# Version that uses JAX optimization library\n",
        "\n",
        "#@jit\n",
        "def sgd_jax(params, loss_fn, batch_stream, max_epochs, opt_init, opt_update, get_params):\n",
        "    loss_history = []\n",
        "    opt_state = opt_init(params)\n",
        "    \n",
        "    #@jit\n",
        "    def update(i, opt_state, batch):\n",
        "        params = get_params(opt_state)\n",
        "        g = grad(loss_fn)(params, batch)\n",
        "        return opt_update(i, g, opt_state) \n",
        "    \n",
        "    print_every = max(1, int(0.1*max_epochs))\n",
        "    total_steps = 0\n",
        "    for epoch in range(max_epochs):\n",
        "        epoch_loss = 0.0\n",
        "        for batch_dict in batch_stream:\n",
        "            X, y = batch_dict[\"X\"], batch_dict[\"y\"]\n",
        "            batch = (X, y)\n",
        "            total_steps += 1\n",
        "            opt_state = update(total_steps, opt_state, batch)\n",
        "        params = get_params(opt_state)\n",
        "        train_loss = np.float(loss_fn(params, batch))\n",
        "        loss_history.append(train_loss)\n",
        "        if epoch % print_every == 0:\n",
        "            print('Epoch {}, batch loss {}'.format(epoch, train_loss))\n",
        "    return params, loss_history"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCOrHGTvbbfC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83562836-cc46-4a09-ceb1-3d5307f30c3f"
      },
      "source": [
        "# JAX with constant LR should match our minimal version of SGD\n",
        "\n",
        "schedule = optimizers.constant(step_size=lr)\n",
        "opt_init, opt_update, get_params = optimizers.sgd(step_size=schedule)\n",
        "\n",
        "w_mle_sgd2, history = sgd_jax(w_init, NLL, batch_stream, max_epochs, \n",
        "                              opt_init, opt_update, get_params)\n",
        "print(w_mle_sgd2)\n",
        "print(history)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, batch loss 0.36490148305892944\n",
            "Epoch 1, batch loss 0.34500643610954285\n",
            "Epoch 2, batch loss 0.32851701974868774\n",
            "Epoch 3, batch loss 0.3143332004547119\n",
            "Epoch 4, batch loss 0.3018316924571991\n",
            "[-0.538 -0.827  0.613  1.661]\n",
            "[0.36490148305892944, 0.34500643610954285, 0.32851701974868774, 0.3143332004547119, 0.3018316924571991]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHHGWDyLfnlk"
      },
      "source": [
        "# Fitting flax models\n",
        "\n",
        "Below we show how to fit a multi-class logistic regression model using flax. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUICitLqjkrR"
      },
      "source": [
        "## Import code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHI0RPrPblpY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bde95e6-132f-4cd1-c411-6b4d7bd4cd01"
      },
      "source": [
        "# Install Flax at head:\n",
        "!pip install --upgrade -q git+https://github.com/google/flax.git"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for flax (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyv9ODiCf_aH"
      },
      "source": [
        "import flax\n",
        "from flax.core import freeze, unfreeze\n",
        "from flax import linen as nn\n",
        "from flax import optim\n",
        "\n",
        "from jax.config import config\n",
        "config.enable_omnistaging() # Linen requires enabling omnistaging"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBzM5HwiiuM6",
        "outputId": "f004a57e-996a-4999-8935-8c008556b5c8"
      },
      "source": [
        "# Book code\n",
        "!git clone https://github.com/probml/pyprobml"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pyprobml'...\n",
            "remote: Enumerating objects: 62, done.\u001b[K\n",
            "remote: Counting objects: 100% (62/62), done.\u001b[K\n",
            "remote: Compressing objects: 100% (56/56), done.\u001b[K\n",
            "remote: Total 5489 (delta 29), reused 16 (delta 6), pack-reused 5427\u001b[K\n",
            "Receiving objects: 100% (5489/5489), 197.46 MiB | 30.28 MiB/s, done.\n",
            "Resolving deltas: 100% (3084/3084), done.\n",
            "Checking out files: 100% (473/473), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKcrB1ZUi03D",
        "outputId": "ab48450f-68db-4893-e909-444b374bc522"
      },
      "source": [
        "!cat pyprobml/scripts/fit_flax.py"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# Fitting functions for flax models\n",
            "\n",
            "\n",
            "import numpy as np\n",
            "import jax\n",
            "import jax.numpy as jnp\n",
            "import pandas as pd\n",
            "\n",
            "\n",
            "def fit_model(model, train_iter, test_iter,  rng,\n",
            "        num_steps, make_optimizer, train_batch, eval_batch,\n",
            "        preprocess_train_batch = None, preprocess_test_batch = None,\n",
            "        print_every = 1, eval_every = 1):\n",
            "  batch = next(train_iter)\n",
            "  X = batch['X']\n",
            "  params = model.init(rng, X)['params']\n",
            "  optimizer = make_optimizer.create(params)  \n",
            "  history = pd.DataFrame({'train_loss': [], 'train_accuracy': [],\n",
            "                   'test_loss': [], 'test_accuracy': [], 'step': []})\n",
            "  \n",
            "  for step in range(num_steps):\n",
            "    batch = next(train_iter)\n",
            "    if preprocess_train_batch is not None:\n",
            "      batch = preprocess_train_batch(batch, rng)\n",
            "    optimizer, train_metrics = train_batch(model, optimizer, batch)\n",
            "    if (print_every > 0) & (step % print_every == 0):\n",
            "       print('train step: {:d}, loss: {:0.4f}, accuracy: {:0.2f}'.format(\n",
            "              step, train_metrics['loss'], \n",
            "                 train_metrics['accuracy']))\n",
            "       \n",
            "    if (eval_every > 0) & (step % eval_every == 0):\n",
            "      batch = next(test_iter)\n",
            "      if preprocess_test_batch is not None:\n",
            "        batch = preprocess_test_batch(batch, rng)\n",
            "      test_metrics = eval_batch(model, optimizer.target, batch)\n",
            "      history = history.append(\n",
            "                {'train_loss': train_metrics['loss'],\n",
            "                'train_accuracy': train_metrics['accuracy'],\n",
            "                'test_loss': test_metrics['loss'],\n",
            "                'test_accuracy': test_metrics['accuracy'],\n",
            "                'step': step},\n",
            "                ignore_index=True\n",
            "                )\n",
            "      \n",
            "  params = optimizer.target\n",
            "  return params, history\n",
            "\n",
            "\n",
            "\n",
            "def onehot(labels, num_classes):\n",
            "  y = (labels[..., None] == jnp.arange(num_classes)[None])\n",
            "  return y.astype(jnp.float32)\n",
            "\n",
            "def cross_entropy_loss_onehot(logits, onehots):\n",
            "  return -jnp.mean(jnp.sum(onehots * logits, axis=-1))\n",
            "\n",
            "def compute_metrics(logits, onehots):\n",
            "  loss = cross_entropy_loss_onehot(logits, onehots)\n",
            "  accuracy = jnp.mean(jnp.argmax(logits, -1) == jnp.argmax(onehots, -1))\n",
            "  metrics = {\n",
            "      'loss': loss,\n",
            "      'accuracy': accuracy,\n",
            "  }\n",
            "  return metrics\n",
            "\n",
            "def eval_batch(model, params, batch):\n",
            "  logits = model.apply({'params': params}, batch['X'])\n",
            "  onehots = onehot(batch['y'], model.nclasses)\n",
            "  return compute_metrics(logits, onehots)\n",
            "\n",
            "eval_batch = jax.jit(eval_batch, static_argnums=0)\n",
            "\n",
            "def train_batch(model, optimizer, batch):\n",
            "  onehots = onehot(batch['y'], model.nclasses)\n",
            "  def loss_fn(params):\n",
            "    logits = model.apply({'params': params}, batch['X'])\n",
            "    loss = cross_entropy_loss_onehot(logits, onehots)\n",
            "    return loss, logits\n",
            "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
            "  (_, logits), grad = grad_fn(optimizer.target)\n",
            "  optimizer = optimizer.apply_gradient(grad)\n",
            "  metrics = compute_metrics(logits, onehots)\n",
            "  return optimizer, metrics\n",
            "\n",
            "train_batch = jax.jit(train_batch, static_argnums=0)\n",
            "\n",
            "\n",
            "############ Testing\n",
            "\n",
            "import flax\n",
            "from flax.core import freeze, unfreeze\n",
            "from flax import linen as nn\n",
            "from flax import optim\n",
            "\n",
            "from jax.config import config\n",
            "config.enable_omnistaging() # Linen requires enabling omnistaging\n",
            "\n",
            "class ModelTest(nn.Module):\n",
            "  nhidden: int\n",
            "  nclasses: int\n",
            "  @nn.compact\n",
            "  def __call__(self, x):\n",
            "    if self.nhidden > 0:\n",
            "      x = nn.Dense(self.nhidden)(x)\n",
            "      x = nn.relu(x)\n",
            "    x = nn.Dense(self.nclasses)(x)\n",
            "    x = nn.log_softmax(x)\n",
            "    return x\n",
            "\n",
            "\n",
            "def fit_model_test():\n",
            "  # We just check we can run the functions and that they return \"something\"\n",
            "  N = 3; D = 5; C = 10;\n",
            "  model = ModelTest(nhidden = 0, nclasses = C)\n",
            "  rng = jax.random.PRNGKey(0)\n",
            "  X = np.random.randn(N,D)\n",
            "  y = np.random.choice(C, size=N, p=(1/C)*np.ones(C));\n",
            "  batch = {'X': X, 'y': y}\n",
            "  params = model.init(rng, X)['params']\n",
            "  metrics = eval_batch(model, params, batch)\n",
            "  make_optimizer = optim.Momentum(learning_rate=0.1, beta=0.9)\n",
            "  optimizer = make_optimizer.create(params)\n",
            "  optimizer, metrics = train_batch(model, optimizer, batch)\n",
            "  #print(optimizer)\n",
            "  num_steps = 2\n",
            "  def make_iter():\n",
            "    while True:\n",
            "      yield batch\n",
            "  train_iter = make_iter(); test_iter = make_iter();\n",
            "  params, history =  fit_model(model, train_iter, test_iter,  rng,\n",
            "      num_steps, make_optimizer, train_batch, eval_batch,\n",
            "      print_every=1)\n",
            "  print('test passed')\n",
            "  \n",
            "  \n",
            "def main():\n",
            "    fit_model_test()\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kXCvH37o58s"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SMa9njai3Qt",
        "outputId": "7161a0ac-a403-4780-90bb-d9bf9a66cb7c"
      },
      "source": [
        "os.chdir('pyprobml/scripts')\n",
        "import fit_flax as ff\n",
        "ff.fit_model_test()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train step: 0, loss: 3.4245, accuracy: 0.00\n",
            "train step: 1, loss: 3.2471, accuracy: 0.00\n",
            "test passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibqUSp6KpH-E"
      },
      "source": [
        "\n",
        "from google.colab import files\n",
        "#files.view('fit_flax.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHnVMv3zjnt3"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a-tDJOfjIf7",
        "outputId": "63ab2bd3-7293-41cf-dee1-70baf911623e"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import sklearn\n",
        "import sklearn.datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def get_datasets_iris():\n",
        "  iris = sklearn.datasets.load_iris()\n",
        "  X = iris[\"data\"]\n",
        "  y = iris[\"target\"] \n",
        "  N, D = X.shape # 150, 4\n",
        "  X_train, X_test, y_train, y_test = train_test_split(\n",
        "          X, y, test_size=0.33, random_state=42)\n",
        "  train_data = {'X': X_train, 'y': y_train}\n",
        "  test_data = {'X': X_test, 'y': y_test}\n",
        "  return train_data, test_data\n",
        "\n",
        "def load_dataset_iris(split, batch_size=None):\n",
        "  train_ds, test_ds = get_datasets_iris()\n",
        "  if split == tfds.Split.TRAIN:\n",
        "    ds = tf.data.Dataset.from_tensor_slices({\"X\": train_ds[\"X\"], \"y\": train_ds[\"y\"]})\n",
        "  elif split == tfds.Split.TEST:\n",
        "    ds = tf.data.Dataset.from_tensor_slices({\"X\": test_ds[\"X\"], \"y\": test_ds[\"y\"]})\n",
        "  if batch_size is not None:\n",
        "    ds = ds.shuffle(buffer_size=batch_size)\n",
        "    ds = ds.batch(batch_size)\n",
        "  else:\n",
        "    N = len(train_ds['X'])\n",
        "    ds = ds.batch(N)\n",
        "  ds = ds.prefetch(buffer_size=5)\n",
        "  ds = ds.repeat() # make infinite stream of data\n",
        "  return iter(tfds.as_numpy(ds)) # python iterator\n",
        "\n",
        "\n",
        "batch_size = 30\n",
        "train_ds = load_dataset_iris(tfds.Split.TRAIN, batch_size)\n",
        "batch = next(train_ds)\n",
        "print(batch['X'].shape)\n",
        "print(batch['y'].shape)\n",
        "\n",
        "test_ds = load_dataset_iris(tfds.Split.TEST, None) # load full test set\n",
        "batch = next(test_ds)\n",
        "print(batch['X'].shape)\n",
        "print(batch['y'].shape)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30, 4)\n",
            "(30,)\n",
            "(50, 4)\n",
            "(50,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrzcCrmsjpi-"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5JQ3iovjqGS"
      },
      "source": [
        "class Model(nn.Module):\n",
        "  nhidden: int\n",
        "  nclasses: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    if self.nhidden > 0:\n",
        "      x = nn.Dense(self.nhidden)(x)\n",
        "      x = nn.relu(x)\n",
        "    x = nn.Dense(self.nclasses)(x)\n",
        "    x = nn.log_softmax(x)\n",
        "    return x"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwuGK8GJjxy_"
      },
      "source": [
        "## Training loop\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "fN29jn7XjzG1",
        "outputId": "874725af-ad23-4fd0-c9a6-2376f8f9eacc"
      },
      "source": [
        "from flax import optim\n",
        "\n",
        "make_optimizer = optim.Momentum(learning_rate=0.1, beta=0.9)\n",
        "\n",
        "model = Model(nhidden = 0, nclasses=3) # no hidden units ie logistic regression\n",
        "\n",
        "batch_size = 100 # 30 # full batch training\n",
        "train_ds = load_dataset_iris(tfds.Split.TRAIN, batch_size)\n",
        "test_ds = load_dataset_iris(tfds.Split.TEST, batch_size)\n",
        "\n",
        "\n",
        "def preprocess_batch(batch, prng_key = None):\n",
        "  X = batch['X']\n",
        "  y = batch['y']\n",
        "  #d = {'X': X, 'y': onehot(y, num_classes=3)}\n",
        "  d = {'X': X, 'y': y}\n",
        "  return d\n",
        "\n",
        "rng = jax.random.PRNGKey(0)\n",
        "num_steps = 200\n",
        "\n",
        "params, history =  ff.fit_model(model, train_ds, test_ds,  rng,\n",
        "        num_steps, make_optimizer,\n",
        "        ff.train_batch, ff.eval_batch, \n",
        "        print_every=50, eval_every=50)\n",
        "  \n",
        "display(history)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train step: 0, loss: 4.2830, accuracy: 0.35\n",
            "train step: 50, loss: 0.1424, accuracy: 0.94\n",
            "train step: 100, loss: 0.1041, accuracy: 0.98\n",
            "train step: 150, loss: 0.0955, accuracy: 0.98\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_loss</th>\n",
              "      <th>train_accuracy</th>\n",
              "      <th>test_loss</th>\n",
              "      <th>test_accuracy</th>\n",
              "      <th>step</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.2829556</td>\n",
              "      <td>0.35</td>\n",
              "      <td>1.8996173</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.14242826</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.13580628</td>\n",
              "      <td>0.9</td>\n",
              "      <td>50.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.10408674</td>\n",
              "      <td>0.97999996</td>\n",
              "      <td>0.086586</td>\n",
              "      <td>1.0</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.09550042</td>\n",
              "      <td>0.97999996</td>\n",
              "      <td>0.08249063</td>\n",
              "      <td>0.97999996</td>\n",
              "      <td>150.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   train_loss train_accuracy   test_loss test_accuracy   step\n",
              "0   4.2829556           0.35   1.8996173          0.28    0.0\n",
              "1  0.14242826           0.94  0.13580628           0.9   50.0\n",
              "2  0.10408674     0.97999996    0.086586           1.0  100.0\n",
              "3  0.09550042     0.97999996  0.08249063    0.97999996  150.0"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "-NzU_wMAkut-",
        "outputId": "f4a5a51c-2348-437d-b7a4-2864f4a7c750"
      },
      "source": [
        "plt.figure()\n",
        "plt.plot(history['step'], history['test_accuracy'], 'o-', label='test accuracy')\n",
        "plt.xlabel('num. minibatches')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "um91hW0ikzfe"
      },
      "source": [
        "## Compare to sklearn\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XPa5V5hk0vd",
        "outputId": "e860d4bf-7f83-416a-ef32-4f707d17f803"
      },
      "source": [
        "train_ds, test_ds = get_datasets_iris()\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# We set C to a large number to turn off regularization.\n",
        "log_reg = LogisticRegression(solver=\"lbfgs\", C=1e3, fit_intercept=True)\n",
        "log_reg.fit(train_ds['X'], train_ds['y'])\n",
        "\n",
        "w_sklearn = np.ravel(log_reg.coef_)\n",
        "print(w_sklearn)\n",
        "b_sklearn = np.ravel(log_reg.intercept_)\n",
        "print(b_sklearn)\n",
        "\n",
        "yprob_sklearn = log_reg.predict_proba(test_ds['X'])\n",
        "print(yprob_sklearn.shape)\n",
        "print(yprob_sklearn[:10,:])\n",
        "\n",
        "\n",
        "ypred_sklearn = jnp.argmax(yprob_sklearn, axis=-1)\n",
        "print(ypred_sklearn.shape)\n",
        "print(ypred_sklearn[:10])"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5.69473 8.89993 -12.90385 -6.59589 -1.40077 1.88896 0.08464 -14.39687\n",
            " -4.29397 -10.78889 12.81921 20.99277]\n",
            "[3.97582 32.52712 -36.50294]\n",
            "(50, 3)\n",
            "[[0.00000 1.00000 0.00000]\n",
            " [1.00000 0.00000 0.00000]\n",
            " [0.00000 0.00000 1.00000]\n",
            " [0.00000 0.99999 0.00000]\n",
            " [0.00001 0.99999 0.00000]\n",
            " [1.00000 0.00000 0.00000]\n",
            " [0.00605 0.99395 0.00000]\n",
            " [0.00000 0.00000 1.00000]\n",
            " [0.00000 0.98867 0.01133]\n",
            " [0.00006 0.99994 0.00000]]\n",
            "(50,)\n",
            "[1 0 2 1 1 0 1 2 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_QxgKCilBrn",
        "outputId": "f32a115b-4f8d-49e6-93e8-54e7e169af61"
      },
      "source": [
        "# Flax version\n",
        "print(params)\n",
        "\n",
        "train_ds, test_ds = get_datasets_iris()\n",
        "Xtest = test_ds['X']\n",
        "logits = model.apply({'params': params}, Xtest)\n",
        "yprob = nn.softmax(logits)\n",
        "print(yprob.shape)\n",
        "print(yprob[:10,:])\n",
        "print(np.allclose(yprob_sklearn, yprob, atol=1e-0)) # very loose numerical tolerance\n",
        "\n",
        "ypred = jnp.argmax(yprob, axis=-1)\n",
        "print(ypred[:10])\n",
        "print(np.allclose(ypred_sklearn, ypred))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FrozenDict({\n",
            "    Dense_0: {\n",
            "        bias: DeviceArray([0.67322, 1.05858, -1.73180], dtype=float32),\n",
            "        kernel: DeviceArray([[1.13125, 0.95521, -2.30099],\n",
            "                     [2.86528, -0.09720, -3.41202],\n",
            "                     [-3.86220, -0.45856, 4.46158],\n",
            "                     [-1.57317, -1.24467, 3.70615]], dtype=float32),\n",
            "    },\n",
            "})\n",
            "(50, 3)\n",
            "[[0.00057 0.94575 0.05368]\n",
            " [0.99750 0.00250 0.00000]\n",
            " [0.00000 0.00014 0.99986]\n",
            " [0.00131 0.91360 0.08509]\n",
            " [0.00045 0.97463 0.02493]\n",
            " [0.99551 0.00449 0.00000]\n",
            " [0.02960 0.96893 0.00147]\n",
            " [0.00008 0.27976 0.72016]\n",
            " [0.00012 0.66911 0.33077]\n",
            " [0.00644 0.98950 0.00406]]\n",
            "True\n",
            "[1 0 2 1 1 0 1 2 1 1]\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}