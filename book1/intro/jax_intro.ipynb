{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jax_intro.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/probml/pyprobml/blob/master/book1/intro/jax_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4bE-S8yDALH"
      },
      "source": [
        "# JAX <a class=\"anchor\" id=\"jax\"></a>\n",
        "\n",
        "[JAX](https://github.com/google/jax) is a  version of Numpy that runs fast on CPU, GPU and TPU.\n",
        "In addition to having a fast backend, JAX supports\n",
        "several useful python-level program transformations:\n",
        "\n",
        "* [vmap](#vmap), vectorized map operator for automatic vectorization or batching.\n",
        "* [autograd](#AD), for automatic differentiation.\n",
        "* [jit](#jit), just in time compiler for speeding up your code (even on a CPU!).\n",
        "\n",
        "We illustrate these below.\n",
        "\n",
        "More details can be found at the \n",
        "* [official JAX quickstart page](https://github.com/google/jax#quickstart-colab-in-the-cloud).\n",
        "* [official JAX quickstart colab](https://colab.research.google.com/github/google/jax/blob/master/docs/notebooks/quickstart.ipynb#scrollTo=SY8mDvEvCGqk)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCI0G3tfDFSs"
      },
      "source": [
        "# Standard Python libraries\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "np.set_printoptions(precision=3)\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "import imageio\n",
        "\n",
        "from IPython import display\n",
        "%matplotlib inline\n",
        "\n",
        "import sklearn\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9kAsUWYDIOk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9822b57-a610-4604-d144-3fca856199b2"
      },
      "source": [
        "\n",
        "# Load JAX\n",
        "import jax\n",
        "import jax.numpy as np\n",
        "import numpy as onp # original numpy\n",
        "from jax import grad, hessian, jit, vmap\n",
        "print(\"jax version {}\".format(jax.__version__))\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jax version 0.2.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXExOyfluzIj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a192e591-d039-461b-9e5e-e296dc46e2b1"
      },
      "source": [
        "# Check if GPU is available\n",
        "!nvidia-smi\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Dec 31 05:41:36 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.27.04    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c4Y-hI3FR8f",
        "outputId": "2882e502-b7f0-4ffe-8d96-ef6c44005ff0"
      },
      "source": [
        "\n",
        "# Check if JAX is using GPU\n",
        "print(\"jax backend {}\".format(jax.lib.xla_bridge.get_backend().platform))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jax backend gpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Zsh5DdOF4R1"
      },
      "source": [
        "# Vmap <a class=\"anchor\" id=\"vmap\"></a>\n",
        "\n",
        "\n",
        "To illustrate vmap, consider a binary logistic regression model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XAMcxMsF0-Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c90b39ef-f67a-437d-fae0-e631474ea288"
      },
      "source": [
        "def sigmoid(x): return 0.5 * (np.tanh(x / 2.) + 1)\n",
        "\n",
        "def predict_single(w, x):\n",
        "    return sigmoid(np.dot(w, x)) # <(D) , (D)> = (1) # inner product\n",
        "  \n",
        "def predict_batch(w, X):\n",
        "    return sigmoid(np.dot(X, w)) # (N,D) * (D,1) = (N,1) # matrix-vector multiply\n",
        "\n",
        "\n",
        "D = 2\n",
        "N = 3\n",
        "onp.random.seed(42)\n",
        "w = onp.random.randn(D)\n",
        "X = onp.random.randn(N, D)\n",
        "y = onp.random.randint(0, 2, N)\n",
        "\n",
        "# We can apply predict_batch to a matrix of data, but we cannot apply predict_single in this way\n",
        "# because the order of the arguments to np.dot is incorrect.\n",
        "\n",
        "p1 = predict_batch(w, X)\n",
        "try:\n",
        "    p2 = predict_single(w, X)\n",
        "except:\n",
        "    print('cannot apply to batch')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cannot apply to batch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-qWaSNBGIqg"
      },
      "source": [
        "To avoid having to think about batch shape, it is often easier to write a function that works on single\n",
        "input vectors. We can then apply this in a loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFqzy2ZFF7Fc"
      },
      "source": [
        "p3 = [predict_single(w, x) for x in X]\n",
        "assert np.allclose(p1, p3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZSksj-1GZcU"
      },
      "source": [
        "Unfortunately, mapping down a list is slow.\n",
        "Fortunately, JAX provides `vmap`, which has the same effect, but can be parallelized.\n",
        "\n",
        "We first apply the `predict_single` function to its first arugment, w, to get a function that only\n",
        "depends on x. We then vectorize this, and map the resulting modified function along rows (dimension 0)\n",
        "of the data matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMNkPb9GGZ1O"
      },
      "source": [
        "from functools import partial\n",
        "\n",
        "predict_single_w = partial(predict_single, w)\n",
        "predict_batch_w = vmap(predict_single_w)\n",
        "p4 = predict_batch_w(X)\n",
        "p5 = vmap(predict_single, in_axes=(None, 0))(w, X)\n",
        "\n",
        "assert np.allclose(p1, p4)\n",
        "assert np.allclose(p1, p5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDXJO4FdUPJM"
      },
      "source": [
        "# Looping constructs\n",
        "\n",
        "Since JAX is functional, it cannot mutate loop counters. So for and while loops need special constructs, as we illustrate below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-wtZiZmU329"
      },
      "source": [
        "## For loops.\n",
        "\n",
        "The semantics of the for loop function in JAX is as follows:\n",
        "```\n",
        "def fori_loop(lower, upper, body_fun, init_val):\n",
        "  val = init_val\n",
        "  for i in range(lower, upper):\n",
        "    val = body_fun(i, val)\n",
        "  return val\n",
        "```\n",
        "We see that ```val``` is used to accumulate the results across iterations.\n",
        "\n",
        "Below is an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIlLD0kKVGya",
        "outputId": "95b3defc-742a-46b5-a86e-95a134e67888"
      },
      "source": [
        "# sum from 1 to N = N*(N+1)/2\n",
        "N = 10\n",
        "s = 0\n",
        "for i in range(1,N+1):\n",
        "  s += i\n",
        "print(s)\n",
        "expected = int(N*(N+1)/2)\n",
        "assert s==expected\n",
        "\n",
        "def body_fun(i, val):\n",
        "  return i + val\n",
        "s2 = jax.lax.fori_loop(1, N+1, body_fun, 0)\n",
        "assert s2==expected\n",
        "\n",
        "s3 = jax.lax.fori_loop(1, N+1, lambda i,val: i+val, 0)\n",
        "assert s3==expected"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "55\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ru5D0tZEiV2e"
      },
      "source": [
        "## While loops\n",
        "\n",
        "Here is the semantics of the JAX while loop\n",
        "\n",
        "\n",
        "```\n",
        "def while_loop(cond_fun, body_fun, init_val):\n",
        "  val = init_val\n",
        "  while cond_fun(val):\n",
        "    val = body_fun(val)\n",
        "  return val\n",
        "```\n",
        "\n",
        "Below is an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWKhfJTDiiSI",
        "outputId": "55a27b06-c921-4701-8036-7109eb18ffd8"
      },
      "source": [
        "N =10\n",
        "s = 0\n",
        "i = 0\n",
        "while (i <= N):\n",
        "  s += i\n",
        "  i += 1\n",
        "print(s)\n",
        "expected = int(N*(N+1)/2)\n",
        "assert s==expected  \n",
        "\n",
        "init_val = (0,0)\n",
        "def cond_fun(val):\n",
        "  s,i = val\n",
        "  return i<=N\n",
        "def body_fun(val):\n",
        "  s,i = val\n",
        "  s += i\n",
        "  i += 1\n",
        "  return (s,i)\n",
        "val = jax.lax.while_loop(cond_fun, body_fun, init_val)\n",
        "s2 = val[0]\n",
        "print(s2)\n",
        "assert s2==expected "
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "55\n",
            "55\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA2RYhnNG9hh"
      },
      "source": [
        "# Autograd <a class=\"anchor\" id=\"AD\"></a>\n",
        "\n",
        "In this section, we illustrate automatic differentation using JAX.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTIybB8b4ar0"
      },
      "source": [
        "## Simple convex functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVY7SewS4qlh"
      },
      "source": [
        "from jax import grad, hessian, jacfwd, jacrev, vmap, jit"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xb0gZ_1HBEyC"
      },
      "source": [
        "Linear function: multi-input, scalar output.\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "f(x; a) &= a^T x\\\\\n",
        "\\nabla_x f(x;a) &= a\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmYqFEs04vkV"
      },
      "source": [
        "# We construct a single output linear function.\n",
        "# In this case, the Jacobian and gradient are the same.\n",
        "Din = 3; Dout = 1;\n",
        "onp.random.seed(42)\n",
        "a = onp.random.randn(Dout, Din)\n",
        "def fun1d(x):\n",
        "    return np.dot(a, x)[0]\n",
        "x = onp.random.randn(Din)\n",
        "g = grad(fun1d)(x)\n",
        "assert np.allclose(g, a)\n",
        "J = jacrev(fun1d)(x)\n",
        "assert np.allclose(J, g)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbgiqkF6BL1E"
      },
      "source": [
        "Linear function: multi-input, multi-output.\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "f(x;A) &= A x \\\\\n",
        "\\nabla_x f(x;A) &= A\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6hkEYxV5EIx"
      },
      "source": [
        "# We construct a multi-output linear function.\n",
        "# We check forward and reverse mode give same Jacobians.\n",
        "Din = 3; Dout = 4;\n",
        "A = onp.random.randn(Dout, Din)\n",
        "def fun(x):\n",
        "    return np.dot(A, x)\n",
        "x = onp.random.randn(Din)\n",
        "Jf = jacfwd(fun)(x)\n",
        "Jr = jacrev(fun)(x)\n",
        "assert np.allclose(Jf, Jr)\n",
        "assert np.allclose(Jf, A)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CN5d-D7XBU9Y"
      },
      "source": [
        "Quadratic form.\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "f(x;A) &= x^T A x \\\\\n",
        "\\nabla_x f(x;A) &= (A+A^T) x \\\\\n",
        "\\nabla^2 x^2 f(x;A) &= A + A^T\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9URZeX8PBbhl"
      },
      "source": [
        "\n",
        "D = 4\n",
        "A = onp.random.randn(D, D)\n",
        "x = onp.random.randn(D)\n",
        "quadfun = lambda x: np.dot(x, np.dot(A, x))\n",
        "\n",
        "J = jacfwd(quadfun)(x)\n",
        "assert np.allclose(J, np.dot(A+A.T, x))\n",
        "\n",
        "H1 = hessian(quadfun)(x)\n",
        "assert np.allclose(H1, A+A.T)\n",
        "\n",
        "def my_hessian(fun):\n",
        "  return jacfwd(jacrev(fun))\n",
        "H2 = my_hessian(quadfun)(x)\n",
        "assert np.allclose(H1, H2)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9ZOhDeqCXu3"
      },
      "source": [
        "Chain rule applied to sigmoid function.\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "f(x;w) &=\\sigma(w^T x) \\\\\n",
        "\\nabla_w f(x;w) &= \\sigma'(w^T x) x \\\\\n",
        "\\sigma(a) &= \\sigma(a) * (1-\\sigma(a)) \n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6q5VfLXLB7rv"
      },
      "source": [
        "\n",
        "\n",
        "onp.random.seed(42)\n",
        "D = 5\n",
        "w = onp.random.randn(D)\n",
        "x = onp.random.randn(D)\n",
        "y = 0 \n",
        "\n",
        "def sigmoid(x): return 0.5 * (np.tanh(x / 2.) + 1)\n",
        "def mu(w): return sigmoid(np.dot(w,x))\n",
        "def deriv_mu(w): return mu(w) * (1-mu(w)) * x\n",
        "deriv_mu_jax =  grad(mu)\n",
        "assert np.allclose(deriv_mu(w), deriv_mu_jax(w))\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeoGcnV54YY9"
      },
      "source": [
        "## Binary logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Isql2l4MGfIt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b2b4a54-ab6e-447f-d2a3-745c18ae37c8"
      },
      "source": [
        "\n",
        "\n",
        "# negative log likelihood\n",
        "def loss(weights, inputs, targets):\n",
        "    preds = predict_batch(weights, inputs)\n",
        "    logprobs = np.log(preds) * targets + np.log(1 - preds) * (1 - targets)\n",
        "    return -np.sum(logprobs)\n",
        "\n",
        "print(loss(w, X, y))\n",
        "\n",
        "# Gradient function\n",
        "grad_fun = grad(loss)\n",
        "\n",
        "# Gradient of each example in the batch - 2 different ways\n",
        "grad_fun_w = partial(grad_fun, w)\n",
        "grads = vmap(grad_fun_w)(X,y)\n",
        "print(grads)\n",
        "assert grads.shape == (N,D)\n",
        "\n",
        "grads2 = vmap(grad_fun, in_axes=(None, 0, 0))(w, X, y) \n",
        "assert np.allclose(grads, grads2)\n",
        "\n",
        "# Gradient for entire batch\n",
        "grad_sum = np.sum(grads, axis=0)\n",
        "assert grad_sum.shape == (D,)\n",
        "print(grad_sum)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.7016186\n",
            "[[-0.306 -0.719]\n",
            " [-0.112 -0.112]\n",
            " [-0.532 -0.258]]\n",
            "[-0.95 -1.09]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3BaHdT4Gj6W",
        "outputId": "1ab5d59b-f1b5-42b2-def4-774969a0222b"
      },
      "source": [
        "# Textbook implementation of gradient\n",
        "def NLL_grad(weights, batch):\n",
        "    X, y = batch\n",
        "    N = X.shape[0]\n",
        "    mu = predict_batch(weights, X)\n",
        "    g = np.sum(np.dot(np.diag(mu - y), X), axis=0)\n",
        "    return g\n",
        "\n",
        "grad_sum_batch = NLL_grad(w, (X,y))\n",
        "print(grad_sum_batch)\n",
        "assert np.allclose(grad_sum, grad_sum_batch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.95 -1.09]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGxDFho3H5ou",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "783ce854-1466-429b-b77f-dc27c33e4839"
      },
      "source": [
        "# We can also compute Hessians, as we illustrate below.\n",
        "from jax import hessian\n",
        "\n",
        "hessian_fun = hessian(loss)\n",
        "\n",
        "# Hessian on one example\n",
        "H0 = hessian_fun(w, X[0,:], y[0])\n",
        "print('Hessian(example 0)\\n{}'.format(H0))\n",
        "\n",
        "# Hessian for batch\n",
        "Hbatch = vmap(hessian_fun, in_axes=(None, 0, 0))(w, X, y) \n",
        "print('Hbatch shape {}'.format(Hbatch.shape))\n",
        "\n",
        "Hbatch_sum = np.sum(Hbatch, axis=0)\n",
        "print('Hbatch sum\\n {}'.format(Hbatch_sum))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hessian(example 0)\n",
            "[[0.105 0.246]\n",
            " [0.246 0.578]]\n",
            "Hbatch shape (3, 2, 2)\n",
            "Hbatch sum\n",
            " [[0.675 0.53 ]\n",
            " [0.53  0.723]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsveS95sIxMh"
      },
      "source": [
        "# Textbook implementation of Hessian\n",
        "\n",
        "def NLL_hessian(weights, batch):\n",
        "  X, y = batch\n",
        "  mu = predict_batch(weights, X)\n",
        "  S = np.diag(mu * (1-mu))\n",
        "  H = np.dot(np.dot(X.T, S), X)\n",
        "  return H\n",
        "\n",
        "H2 = NLL_hessian(w, (X,y) )\n",
        "assert np.allclose(Hbatch_sum, H2, atol=1e-2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnOIJRGxJigp"
      },
      "source": [
        "\n",
        "# JIT (just in time compilation) <a class=\"anchor\" id=\"JIT\"></a>\n",
        "\n",
        "In this section, we illustrate how to use the Jax JIT compiler to make code go faster (even on a CPU). However, it does not work on arbitrary Python code, as we explain below.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsaHkNovICfd"
      },
      "source": [
        "grad_fun_jit = jit(grad_fun) # speedup gradient function\n",
        "grads_jit = vmap(partial(grad_fun_jit, w))(X,y)\n",
        "assert np.allclose(grads, grads_jit)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGZUbPDLKIkv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72c0a11d-0528-49a7-c91c-2636c1ca216a"
      },
      "source": [
        "# We can apply JIT to non ML applications as well.\n",
        "\n",
        "def slow_f(x):\n",
        "  # Element-wise ops see a large benefit from fusion\n",
        "  return x * x + x * 2.0\n",
        "\n",
        "x = np.ones((5000, 5000))\n",
        "fast_f = jit(slow_f)\n",
        "%timeit -n10 -r3 fast_f(x)  \n",
        "%timeit -n10 -r3 slow_f(x)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The slowest run took 63.95 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "10 loops, best of 3: 234 Âµs per loop\n",
            "The slowest run took 4.80 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "10 loops, best of 3: 5.04 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77-33YadKNni"
      },
      "source": [
        "We can also add the `%jit` decorator in front of a function.\n",
        "\n",
        "Note that JIT compilation requires that the control flow through the function  can be determined by the shape (but not concrete value) of its inputs. The function below violates this, since when x<3, it takes one branch, whereas when x>= 3, it takes the other."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ps1W8LhKKj9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f493189-a75c-40f1-c264-cf2ad0a49983"
      },
      "source": [
        "@jit\n",
        "def f(x):\n",
        "  if x < 3:\n",
        "    return 3. * x ** 2\n",
        "  else:\n",
        "    return -4 * x\n",
        "\n",
        "# This will fail!\n",
        "try:\n",
        "  print(f(2))\n",
        "except Exception as e:\n",
        "  print(\"ERROR:\", e)\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR: Abstract tracer value encountered where concrete value is expected.\n",
            "\n",
            "The problem arose with the `bool` function. \n",
            "\n",
            "While tracing the function f at <ipython-input-43-3da05647f18e>:1, this concrete value was not available in Python because it depends on the value of the arguments to f at <ipython-input-43-3da05647f18e>:1 at flattened positions [0], and the computation of these values is being staged out (that is, delayed rather than executed eagerly).\n",
            "\n",
            "You can use transformation parameters such as `static_argnums` for `jit` to avoid tracing particular arguments of transformed functions, though at the cost of more recompiles.\n",
            "\n",
            "See https://jax.readthedocs.io/en/latest/faq.html#abstract-tracer-value-encountered-where-concrete-value-is-expected-error for more information.\n",
            "\n",
            "Encountered tracer value: Traced<ShapedArray(bool[])>with<DynamicJaxprTrace(level=0/1)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tHh4tcXKTRf"
      },
      "source": [
        "We can fix this by telling JAX to trace the control flow through the function using concrete values of some of its arguments. JAX will then compile different versions, depending on the input values. See below for an example.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMaRplccKRHc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3321293-e37a-446a-daaa-fca6e990e6f4"
      },
      "source": [
        "def f(x):\n",
        "  if x < 3:\n",
        "    return 3. * x ** 2\n",
        "  else:\n",
        "    return -4 * x\n",
        "\n",
        "f2 = jit(f, static_argnums=(0,))\n",
        "\n",
        "print(f2(5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_Fkav2GRgnc"
      },
      "source": [
        "Unfortunately, the static argnum method fails with vmap, which passes in different inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHLTdXCSQ6sZ",
        "outputId": "2c6676ac-8bdc-4caf-e4f5-57a9def31680"
      },
      "source": [
        "\n",
        "xs = np.arange(5)\n",
        "try:\n",
        "  ys = vmap(f)(xs)\n",
        "  print('used vmap')\n",
        "except:\n",
        "  ys = np.array([f(x) for x in xs])\n",
        "  print('did not use vmap')\n",
        "print(ys)\n",
        "\n",
        "print(np.sum(ys))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "did not use vmap\n",
            "[  0.   3.  12. -12. -16.]\n",
            "-13.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2zrYp3-KcMc"
      },
      "source": [
        "There are a few other subtleties. If your function has global side-effects, JAX's tracer can cause weird things to happen. A common gotcha is trying to print arrays inside jit'd functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LC5q9OCSTAKQ",
        "outputId": "d385fdd4-770d-43f6-c8ad-fd77e6c0efa4"
      },
      "source": [
        "def f(x):\n",
        "  print(x)\n",
        "  y = 2 * x\n",
        "  print(y)\n",
        "  return y\n",
        "y1 = f(2)\n",
        "print(y1)\n",
        "\n",
        "@jit\n",
        "def f(x):\n",
        "  print(x)\n",
        "  y = 2 * x\n",
        "  print(y)\n",
        "  return y\n",
        "y2 = f(2)\n",
        "print(y2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "4\n",
            "4\n",
            "Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace(level=0/1)>\n",
            "Traced<ShapedArray(int32[])>with<DynamicJaxprTrace(level=0/1)>\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZ0T4PxnKgBN"
      },
      "source": [
        "# A few differences from Numpy\n",
        "\n",
        "Below we list a few items where Jax differs from Numpy.\n",
        "See also the official [list of common gotchas](https://colab.research.google.com/github/google/jax/blob/master/notebooks/Common_Gotchas_in_JAX.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUOZdeYBKjWc"
      },
      "source": [
        "## Random number generation\n",
        "\n",
        "The API for Jax is basically identical to Numpy, except for pseudo random number\n",
        "generation (PRNG).\n",
        "This is because Jax does not maintain any global state, i.e., it is purely functional.\n",
        "This design \"provides reproducible results invariant to compilation boundaries and backends,\n",
        "while also maximizing performance by enabling vectorized generation and parallelization across random calls\"\n",
        "(to quote [the official page](https://github.com/google/jax#a-brief-tour)).\n",
        "                              \n",
        "Thus, whenever we do anything stochastic, we need to give it a fresh RNG key. We can do this by splitting the existing key into pieces. We can do this indefinitely, as shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcTYfznjKeHC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0c8127d-be5f-4306-d779-b25f06eb85b6"
      },
      "source": [
        "import jax.random as random\n",
        "\n",
        "key = random.PRNGKey(0)\n",
        "print(random.normal(key, shape=(3,)))  # [ 1.81608593 -0.48262325  0.33988902]\n",
        "print(random.normal(key, shape=(3,)))  # [ 1.81608593 -0.48262325  0.33988902]  ## identical results\n",
        "\n",
        "# To make a new key, we split the current key into two pieces.\n",
        "key, subkey = random.split(key)\n",
        "print(random.normal(subkey, shape=(3,)))  # [ 1.1378783  -1.22095478 -0.59153646]\n",
        "\n",
        "# We can continue to split off new pieces from the global key.\n",
        "key, subkey = random.split(key)\n",
        "print(random.normal(subkey, shape=(3,)))  # [-0.06607265  0.16676566  1.17800343]\n",
        "\n",
        "# We can always use original numpy if we like (although this may interfere with the deterministic behavior of jax)\n",
        "onp.random.seed(42)\n",
        "print(onp.random.randn(3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1.816 -0.483  0.34 ]\n",
            "[ 1.816 -0.483  0.34 ]\n",
            "[ 1.138 -1.221 -0.592]\n",
            "[-0.066  0.167  1.178]\n",
            "[ 0.497 -0.138  0.648]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwmI9DH2K_nl"
      },
      "source": [
        "## Implicitly casting lists to vectors\n",
        "\n",
        "You cannot treat a list of numbers as a vector. Instead you must explicitly create the vector using the np.array() constructor.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUURw01jKnXC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd8eb7d3-9432-41d3-e0ed-734e0c7ecb04"
      },
      "source": [
        "# You cannot treat a list of numbers as a vector. \n",
        "try:\n",
        "  S = np.diag([1.0, 2.0, 3.0])\n",
        "except:\n",
        "  print('must convert indices to np.array')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "must convert indices to np.array\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mMgaegpLCYw"
      },
      "source": [
        "# Instead you should explicitly construct the vector.\n",
        "\n",
        "S = np.diag(np.array([1.0, 2.0, 3.0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AieugSOZLGi5"
      },
      "source": [
        "## Mutation of arrays \n",
        "\n",
        "Since JAX is functional, you cannot mutate arrays in place,\n",
        "since this makes program analysis and transformation very difficult. JAX requires a pure functional expression of a numerical program.\n",
        "Instead, JAX offers the functional update functions: `index_update`, `index_add`, `index_min`, `index_max`, and the `index` helper. These are illustrated below. \n",
        "\n",
        "Note: If the input values of `index_update` aren't reused, jit-compiled code will perform these operations in-place, rather than making a copy. \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEKfhvrTLEBf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bb751a7-7f3c-4498-da7a-db048b110c54"
      },
      "source": [
        "# You cannot assign directly to elements of an array.\n",
        "\n",
        "jax_array = np.zeros((3,3), dtype=np.float32)\n",
        "\n",
        "# In place update of JAX's array will yield an error!\n",
        "try:\n",
        "  jax_array[1, :] = 1.0\n",
        "except:\n",
        "  print('must use index_update')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "must use index_update\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBi15EAcLIru",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "0546aab4-12bb-4aa9-d983-8744079b78ca"
      },
      "source": [
        "from jax.ops import index, index_add, index_update\n",
        "\n",
        "jax_array = np.zeros((3, 3))\n",
        "print(\"original array:\")\n",
        "print(jax_array)\n",
        "\n",
        "new_jax_array = index_update(jax_array, index[1, :], 1.)\n",
        "\n",
        "new_jax_array2 = index_add(new_jax_array, index[:, 2], 7.)\n",
        "print(\"new array post update\")\n",
        "print(new_jax_array)\n",
        "\n",
        "print(\"new array post add\")\n",
        "print(new_jax_array2)\n",
        "\n",
        "print(\"old array unchanged:\")\n",
        "print(jax_array)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original array:\n",
            "[[0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]]\n",
            "new array post update\n",
            "[[0. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "new array post add\n",
            "[[0. 0. 7.]\n",
            " [1. 1. 8.]\n",
            " [0. 0. 7.]]\n",
            "old array unchanged:\n",
            "[[0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6G5zdgRxGUE3"
      },
      "source": [
        "# JAX neural net libraries\n",
        "\n",
        "JAX is a purely functional library, which differs from Tensorflow and\n",
        "Pytorch, which are stateful. The main advantages of functional programming\n",
        "are that  we can safely transform the code, and/or run it in parallel, without worrying about\n",
        "global state changing behind the scenes. The main disadvantage is that code (especially DNNs) can be harder to write.\n",
        "To simplify the task, various DNN libraries have been designed, as we list below. In this book, we use Flax.\n",
        "\n",
        "|Name|Description|\n",
        "|----|----|\n",
        "|[Stax](https://github.com/google/jax/blob/master/jax/experimental/stax.py)|Barebones DNN DSL|\n",
        "|[Flax](https://github.com/google/flax)|DNN library for creating and training models|\n",
        "|[Haiku](https://github.com/deepmind/dm-haiku)|DNN library for creating models|\n",
        "|[Trax](https://github.com/google/trax)|DNN library, focus on sequence models|\n",
        "|[Objax](https://github.com/google/objax)|DNN framework, similar to PyTorch, not compatible with other JAX libraries|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-JiigmlGeBB"
      },
      "source": [
        "# Other JAX  libraries\n",
        "\n",
        "There are many other useful JAX libraries, most of which are purely functional, and therefore compose nicely.\n",
        "\n",
        "|Name|Description|\n",
        "|----|----|\n",
        "|[NumPyro](https://github.com/pyro-ppl/numpyro)|Library for (deep) probabilistic modeling|\n",
        "|[Optax](https://github.com/deepmind/optax)|Library for defining gradient-based optimizers|\n",
        "|[RLax](https://github.com/deepmind/rlax)|Library for reinforcement learning|\n",
        "|[Chex](https://github.com/deepmind/chex)|Library for debugging and developing reliable JAX code|"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7U9Ee15LLC-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}