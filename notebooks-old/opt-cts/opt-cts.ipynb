{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "opt.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/probml/pyprobml/blob/master/notebooks/opt-cts/opt-cts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b520E1nCIBHc",
        "colab_type": "text"
      },
      "source": [
        "# Optimization\n",
        "\n",
        "In this notebook, we explore various  algorithms\n",
        "for solving x* = argmin_{x in R^D} f(x), where f(x) is a differentiable cost function.\n",
        "\n",
        "## TOC\n",
        "* [Automatic differentiation](#AD)\n",
        "* [Second-order full-batch optimization](#second)\n",
        "* [Stochastic gradient descent](#SGD)\n",
        "* [TF2 tutorial by  Mukesh Mithrakumar](https://nbviewer.jupyter.org/github/adhiraiyan/DeepLearningWithTF2.0/blob/master/notebooks/04.00-Numerical-Computation.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeuOgABaIENZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn\n",
        "import scipy\n",
        "import scipy.optimize\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits import mplot3d\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import itertools\n",
        "import time\n",
        "from functools import partial\n",
        "\n",
        "import os\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from scipy.special import logsumexp\n",
        "np.set_printoptions(precision=3)\n",
        "\n",
        "\n",
        "# We make some wrappers around random number generation\n",
        "# so it works even if we switch from numpy to JAX\n",
        "import numpy as onp # original numpy\n",
        "\n",
        "def set_seed(seed):\n",
        "    onp.random.seed(seed)\n",
        "    \n",
        "def randn(*args):\n",
        "    return onp.random.randn(*args)\n",
        "        \n",
        "def randperm(args):\n",
        "    return onp.random.permutation(args)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPozRwDAKFb8",
        "colab_type": "code",
        "outputId": "2fa4e065-3a50-4b64-ec1e-cdb1ae66b0ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "print(\"torch version {}\".format(torch.__version__))\n",
        "if torch.cuda.is_available():\n",
        "  print(torch.cuda.get_device_name(0))\n",
        "  print(\"current device {}\".format(torch.cuda.current_device()))\n",
        "else:\n",
        "  print(\"Torch cannot find GPU\")\n",
        "\n",
        "def set_seed(seed):\n",
        "  onp.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "#torch.backends.cudnn.benchmark = True"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch version 1.1.0\n",
            "Tesla T4\n",
            "current device 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdIhsILJKDPl",
        "colab_type": "code",
        "outputId": "13c1adbd-4310-40f0-fc7c-d038815787d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "\n",
        "# Tensorflow 2.0\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "print(\"tf version {}\".format(tf.__version__))\n",
        "if tf.test.is_gpu_available():\n",
        "    print(tf.test.gpu_device_name())\n",
        "else:\n",
        "    print(\"TF cannot find GPU\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "tf version 2.0.0-beta1\n",
            "/device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNQHpyKLIx_P",
        "colab_type": "code",
        "outputId": "c5a071ff-16f2-4591-fa31-1290940e432f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "# JAX (https://github.com/google/jax)\n",
        "!pip install --upgrade -q https://storage.googleapis.com/jax-releases/cuda$(echo $CUDA_VERSION | sed -e 's/\\.//' -e 's/\\..*//')/jaxlib-$(pip search jaxlib | grep -oP '[0-9\\.]+' | head -n 1)-cp36-none-linux_x86_64.whl\n",
        "!pip install --upgrade -q jax\n",
        "\n",
        "import jax\n",
        "import jax.numpy as np\n",
        "import numpy as onp\n",
        "from jax.scipy.special import logsumexp\n",
        "from jax import grad, hessian, jacfwd, jacrev, jit, vmap\n",
        "from jax.experimental import optimizers\n",
        "print(\"jax version {}\".format(jax.__version__))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jax version 0.1.43\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pIgD7iRLUBt",
        "colab_type": "text"
      },
      "source": [
        "# Automatic differentiation <a class=\"anchor\" id=\"AD\"></a>\n",
        "\n",
        "In this section we  illustrate various AD libraries by using them to derive the gradient of the negative log likelihood for binary logistic regression applied to the Iris dataset. We compare to the manual numpy implementation. \n",
        "\n",
        "As a minor detail, we evaluate the gradient of the NLL of the test data with the parameters set to their training MLE, in order to get an interesting signal; using a random weight vector makes the dynamic range of the output harder to see."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3fX16J4IoL_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit the model to a dataset, so we have an \"interesting\" parameter vector to use.\n",
        "\n",
        "import sklearn.datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "iris = sklearn.datasets.load_iris()\n",
        "X = iris[\"data\"]\n",
        "y = (iris[\"target\"] == 2).astype(onp.int)  # 1 if Iris-Virginica, else 0'\n",
        "N, D = X.shape # 150, 4\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# We set C to a large number to turn off regularization.\n",
        "# We don't fit the bias term to simplify the comparison below.\n",
        "log_reg = LogisticRegression(solver=\"lbfgs\", C=1e5, fit_intercept=False)\n",
        "log_reg.fit(X_train, y_train)\n",
        "w_mle_sklearn = np.ravel(log_reg.coef_)\n",
        "w = w_mle_sklearn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS5AB9NjLZ_i",
        "colab_type": "code",
        "outputId": "52ab27d4-faae-4e43-caf5-a29e4f4beb47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "## Compute gradient of loss \"by hand\" using numpy\n",
        "\n",
        "\n",
        "def BCE_with_logits(logits, targets):\n",
        "    N = logits.shape[0]\n",
        "    logits = logits.reshape(N,1)\n",
        "    logits_plus = np.hstack([np.zeros((N,1)), logits]) # e^0=1\n",
        "    logits_minus = np.hstack([np.zeros((N,1)), -logits])\n",
        "    logp1 = -logsumexp(logits_minus, axis=1)\n",
        "    logp0 = -logsumexp(logits_plus, axis=1)\n",
        "    logprobs = logp1 * targets + logp0 * (1-targets)\n",
        "    return -np.sum(logprobs)/N\n",
        "\n",
        "# Compute using numpy\n",
        "def sigmoid(x): return 0.5 * (np.tanh(x / 2.) + 1)\n",
        "\n",
        "def predict_logit(weights, inputs):\n",
        "    return np.dot(inputs, weights) # Already vectorized\n",
        "\n",
        "def predict_prob(weights, inputs):\n",
        "    return sigmoid(predict_logit(weights, inputs))\n",
        "\n",
        "def NLL(weights, batch):\n",
        "    X, y = batch\n",
        "    logits = predict_logit(weights, X)\n",
        "    return BCE_with_logits(logits, y)\n",
        "\n",
        "def NLL_grad(weights, batch):\n",
        "    X, y = batch\n",
        "    N = X.shape[0]\n",
        "    mu = predict_prob(weights, X)\n",
        "    g = np.sum(np.dot(np.diag(mu - y), X), axis=0)/N\n",
        "    return g\n",
        "\n",
        "y_pred = predict_prob(w, X_test)\n",
        "loss = NLL(w, (X_test, y_test))\n",
        "grad_np = NLL_grad(w, (X_test, y_test))\n",
        "print(\"params {}\".format(w))\n",
        "#print(\"pred {}\".format(y_pred))\n",
        "print(\"loss {}\".format(loss))\n",
        "print(\"grad {}\".format(grad_np))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "params [-4.414 -9.111  6.539 12.686]\n",
            "loss 0.11824002861976624\n",
            "grad [-0.235 -0.122 -0.198 -0.064]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLyk46HbLhgT",
        "colab_type": "text"
      },
      "source": [
        "## AD in JAX  <a class=\"anchor\" id=\"AD-jax\"></a>\n",
        "\n",
        "Below we use JAX to compute the gradient of the NLL for binary logistic regression.\n",
        "For some examples of using JAX to compute the gradients, Jacobians and Hessians of simple linear and quadratic functions,\n",
        "see [this notebook](https://github.com/probml/pyprobml/blob/master/notebooks/linear_algebra.ipynb#AD-jax).\n",
        "More details on JAX's autodiff can be found in the official [autodiff cookbook](https://github.com/google/jax/blob/master/notebooks/autodiff_cookbook.ipynb).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GkR1yHNLcjU",
        "colab_type": "code",
        "outputId": "a9b40018-12b2-4372-914d-357d266db690",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "grad_jax = grad(NLL)(w, (X_test, y_test))\n",
        "print(\"grad {}\".format(grad_jax))\n",
        "assert np.allclose(grad_np, grad_jax)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "grad [-0.235 -0.122 -0.198 -0.064]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxxlnRuHLsMb",
        "colab_type": "text"
      },
      "source": [
        "## AD in Tensorflow   <a class=\"anchor\" id=\"AD-TF\"></a>\n",
        "\n",
        "We just wrap the relevant forward computations inside GradientTape(), and then call tape.gradient(objective, [variables])."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKvzU6iuLpKg",
        "colab_type": "code",
        "outputId": "578980a7-0e05-4879-aec9-6d69ec9c0def",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "\n",
        "w_tf = tf.Variable(np.reshape(w, (D,1)))  \n",
        "x_test_tf = tf.convert_to_tensor(X_test, dtype=np.float64) \n",
        "y_test_tf = tf.convert_to_tensor(np.reshape(y_test, (-1,1)), dtype=np.float64)\n",
        "with tf.GradientTape() as tape:\n",
        "    logits = tf.linalg.matmul(x_test_tf, w_tf)\n",
        "    y_pred = tf.math.sigmoid(logits)\n",
        "    loss_batch = tf.nn.sigmoid_cross_entropy_with_logits(y_test_tf, logits)\n",
        "    loss_tf = tf.reduce_mean(loss_batch, axis=0)\n",
        "grad_tf = tape.gradient(loss_tf, [w_tf])\n",
        "grad_tf = grad_tf[0][:,0].numpy()\n",
        "assert np.allclose(grad_np, grad_tf)\n",
        "\n",
        "print(\"params {}\".format(w_tf))\n",
        "#print(\"pred {}\".format(y_pred))\n",
        "print(\"loss {}\".format(loss_tf))\n",
        "print(\"grad {}\".format(grad_tf))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0826 04:28:46.621946 140039241475968 deprecation.py:323] From /tensorflow-2.0.0b1/python3.6/tensorflow/python/ops/nn_impl.py:182: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "params <tf.Variable 'Variable:0' shape=(4, 1) dtype=float64, numpy=\n",
            "array([[-4.414],\n",
            "       [-9.111],\n",
            "       [ 6.539],\n",
            "       [12.686]])>\n",
            "loss [0.118]\n",
            "grad [-0.235 -0.122 -0.198 -0.064]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is7yJlgsL4BT",
        "colab_type": "text"
      },
      "source": [
        "## AD in PyTorch  <a class=\"anchor\" id=\"AD-pytorch\"></a>\n",
        "\n",
        "We just compute the objective, call backward() on it, and then lookup variable.grad. However, we have to specify the requires_grad=True attribute on the variable before computing the objective, so that Torch knows to record its values on its tape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0L5NxIaVLu64",
        "colab_type": "code",
        "outputId": "70c4ba8f-f5a0-4019-a85b-944ab54db8ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "w_torch = torch.Tensor(np.reshape(w, [D, 1])).to(device)\n",
        "w_torch.requires_grad_() \n",
        "x_test_tensor = torch.Tensor(X_test).to(device)\n",
        "y_test_tensor = torch.Tensor(y_test).to(device)\n",
        "y_pred = torch.sigmoid(torch.matmul(x_test_tensor, w_torch))[:,0]\n",
        "criterion = torch.nn.BCELoss(reduction='mean')\n",
        "loss_torch = criterion(y_pred, y_test_tensor)\n",
        "loss_torch.backward()\n",
        "grad_torch = w_torch.grad[:,0].cpu().numpy()\n",
        "assert np.allclose(grad_np, grad_torch)\n",
        "\n",
        "print(\"params {}\".format(w_torch))\n",
        "#print(\"pred {}\".format(y_pred))\n",
        "print(\"loss {}\".format(loss_torch))\n",
        "print(\"grad {}\".format(grad_torch))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "params tensor([[-4.4138],\n",
            "        [-9.1106],\n",
            "        [ 6.5387],\n",
            "        [12.6857]], device='cuda:0', requires_grad=True)\n",
            "loss 0.11824004352092743\n",
            "grad [-0.235 -0.122 -0.198 -0.064]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BXji_6BL87s",
        "colab_type": "text"
      },
      "source": [
        "# Second-order, full-batch optimization <a class=\"anchor\" id=\"second\"></a>\n",
        "\n",
        "The \"gold standard\" of optimization is second-order methods, that leverage Hessian information. Since the Hessian has O(D^2) parameters, such methods do not scale to high-dimensional problems. However, we can sometimes approximate the Hessian using low-rank or diagonal approximations. Below we illustrate the low-rank BFGS method, and the limited-memory version of BFGS, that uses O(D H) space and O(D^2) time per step, where H is the history length.\n",
        "\n",
        "In general, second-order methods also require exact (rather than noisy) gradients. In the context of ML, this means they are \"full batch\" methods, since computing the exact gradient requires evaluating the loss on all the datapoints. However, for small data problems, this is feasible (and advisable).\n",
        "\n",
        "Below we illustrate how to use LBFGS as implemented in various libraries.\n",
        "Other second-order optimizers have a similar API.\n",
        "We use the same binary logistic regression problem as above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7FB0U_hL6Qa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Repeat relevant code from AD section above, for convenience.\n",
        "\n",
        "# Dataset\n",
        "import sklearn.datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "iris = sklearn.datasets.load_iris()\n",
        "X = iris[\"data\"]\n",
        "y = (iris[\"target\"] == 2).astype(onp.int)  # 1 if Iris-Virginica, else 0'\n",
        "N, D = X.shape # 150, 4\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "\n",
        "# Sklearn estimate\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "log_reg = LogisticRegression(solver=\"lbfgs\", C=1e5, fit_intercept=False)\n",
        "log_reg.fit(X_train, y_train)\n",
        "w_mle_sklearn = np.ravel(log_reg.coef_)\n",
        "w = w_mle_sklearn\n",
        "\n",
        "# Define Model and binary cross entropy loss \n",
        "def BCE_with_logits(logits, targets):\n",
        "    N = logits.shape[0]\n",
        "    logits = logits.reshape(N,1)\n",
        "    logits_plus = np.hstack([np.zeros((N,1)), logits]) # e^0=1\n",
        "    logits_minus = np.hstack([np.zeros((N,1)), -logits])\n",
        "    logp1 = -logsumexp(logits_minus, axis=1)\n",
        "    logp0 = -logsumexp(logits_plus, axis=1)\n",
        "    logprobs = logp1 * targets + logp0 * (1-targets)\n",
        "    return -np.sum(logprobs)/N\n",
        "\n",
        "def sigmoid(x): return 0.5 * (np.tanh(x / 2.) + 1)\n",
        "\n",
        "def predict_logit(weights, inputs):\n",
        "    return np.dot(inputs, weights) # Already vectorized\n",
        "\n",
        "def predict_prob(weights, inputs):\n",
        "    return sigmoid(predict_logit(weights, inputs))\n",
        "\n",
        "def NLL(weights, batch):\n",
        "    X, y = batch\n",
        "    logits = predict_logit(weights, X)\n",
        "    return BCE_with_logits(logits, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P65drwynMC1s",
        "colab_type": "text"
      },
      "source": [
        "## Scipy version\n",
        "\n",
        "We show how to use the implementation from  [scipy.optimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize)\n",
        "                                   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkTaK-WZMAGL",
        "colab_type": "code",
        "outputId": "63392143-b78e-43b5-c3f3-d1f94e3c26ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import scipy.optimize\n",
        "\n",
        "# We manually compute gradients, but could use Jax instead\n",
        "def NLL_grad(weights, batch):\n",
        "    X, y = batch\n",
        "    N = X.shape[0]\n",
        "    mu = predict_prob(weights, X)\n",
        "    g = np.sum(np.dot(np.diag(mu - y), X), axis=0)/N\n",
        "    return g\n",
        "\n",
        "def training_loss(w):\n",
        "    return NLL(w, (X_train, y_train))\n",
        "\n",
        "def training_grad(w):\n",
        "    return NLL_grad(w, (X_train, y_train))\n",
        "\n",
        "set_seed(0)\n",
        "w_init = randn(D)\n",
        "\n",
        "options={'disp': None,   'maxfun': 1000, 'maxiter': 1000}\n",
        "method = 'BFGS'\n",
        "w_mle_scipy = scipy.optimize.minimize(\n",
        "    training_loss, w_init, jac=training_grad,\n",
        "    method=method, options=options).x   \n",
        "\n",
        "print(\"parameters from sklearn {}\".format(w_mle_sklearn))\n",
        "print(\"parameters from scipy-bfgs {}\".format(w_mle_scipy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "parameters from sklearn [-4.414 -9.111  6.539 12.686]\n",
            "parameters from scipy-bfgs [-4.417 -9.117  6.543 12.695]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5cLYkceMG7A",
        "colab_type": "code",
        "outputId": "5b66c47e-3f4e-4fdd-9a04-ea2928b1e1c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Limited memory version requires that we work with 64bit, since implemented in Fortran.\n",
        "\n",
        "def training_loss2(w):\n",
        "    l = NLL(w, (X_train, y_train))\n",
        "    return onp.float64(l)\n",
        "\n",
        "def training_grad2(w):\n",
        "    g = NLL_grad(w, (X_train, y_train))\n",
        "    return onp.asarray(g, dtype=onp.float64)\n",
        "                 \n",
        "set_seed(0)\n",
        "w_init = randn(D)\n",
        "memory = 10\n",
        "options={'disp': None, 'maxcor': memory,  'maxfun': 1000, 'maxiter': 1000}\n",
        "# The code also handles bound constraints, hence the name\n",
        "method = 'L-BFGS-B'\n",
        "w_mle_scipy = scipy.optimize.minimize(training_loss, w_init, jac=training_grad2, method=method).x \n",
        "\n",
        "\n",
        "print(\"parameters from sklearn {}\".format(w_mle_sklearn))\n",
        "print(\"parameters from scipy-lbfgs {}\".format(w_mle_scipy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "parameters from sklearn [-4.414 -9.111  6.539 12.686]\n",
            "parameters from scipy-lbfgs [-4.415 -9.114  6.54  12.691]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Tax4tIRMM-P",
        "colab_type": "text"
      },
      "source": [
        "## PyTorch version\n",
        "\n",
        "We show how to use the version from  [PyTorch.optim.lbfgs](https://github.com/pytorch/pytorch/blob/master/torch/optim/lbfgs.py).\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Osy0rGuwMKKh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Put data into PyTorch format.\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "N, D = X_train.shape\n",
        "x_train_tensor = torch.Tensor(X_train)\n",
        "y_train_tensor = torch.Tensor(y_train)\n",
        "data_set = TensorDataset(x_train_tensor, y_train_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ph0wKWedMP9K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define model and loss.\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.linear = torch.nn.Linear(D, 1, bias=False) \n",
        "        \n",
        "    def forward(self, x):\n",
        "        y_pred = torch.sigmoid(self.linear(x))\n",
        "        return y_pred\n",
        "    \n",
        "set_seed(0)\n",
        "model = Model() \n",
        "criterion = torch.nn.BCELoss(reduction='mean')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3V69qeYvMRo-",
        "colab_type": "code",
        "outputId": "d01f893d-e1d5-41c1-cbc1-56311dd38b45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "optimizer = torch.optim.LBFGS(model.parameters(), history_size=10)\n",
        "    \n",
        "def closure():\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = model(x_train_tensor)\n",
        "    loss = criterion(y_pred, y_train_tensor)\n",
        "    #print('loss:', loss.item())\n",
        "    loss.backward()\n",
        "    return loss\n",
        "\n",
        "max_iter = 10\n",
        "for i in range(max_iter):\n",
        "    loss = optimizer.step(closure)\n",
        "\n",
        "params = list(model.parameters())\n",
        "w_torch_bfgs = params[0][0].detach().numpy() #(D,) vector\n",
        "print(\"parameters from sklearn {}\".format(w_mle_sklearn))\n",
        "print(\"parameters from torch-bfgs {}\".format(w_torch_bfgs))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "parameters from sklearn [-4.414 -9.111  6.539 12.686]\n",
            "parameters from torch-bfgs [-4.415 -9.114  6.54  12.691]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryBekSr0MWab",
        "colab_type": "text"
      },
      "source": [
        "## TF version\n",
        "    \n",
        "There is also a version of [LBFGS in TF](https://www.tensorflow.org/probability/api_docs/python/tfp/optimizer/lbfgs_minimize) \n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiZXds_DMj31",
        "colab_type": "text"
      },
      "source": [
        "# Stochastic gradient descent <a class=\"anchor\" id=\"SGD\"></a>\n",
        "\n",
        "In this section we  illustrate how to implement SGD. We apply it to a simple convex problem, namely MLE for binary logistic regression on the small iris dataset, so we can compare to the exact batch methods we illustrated above.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtOeheP-MnB7",
        "colab_type": "text"
      },
      "source": [
        "## Numpy version\n",
        "We show a minimal implementation of SGD using vanilla numpy. For convenience, we use TFDS to create a stream of mini-batches. We compute gradients by hand, but can use any AD library.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikgayvv6RwiQ",
        "colab_type": "code",
        "outputId": "c7c14bf2-3577-4a76-8d4e-5fb33a4d12eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "\n",
        "def make_batcher(batch_size, X, y):\n",
        "  def get_batches():\n",
        "    # Convert numpy arrays to tfds\n",
        "    ds = tf.data.Dataset.from_tensor_slices({\"X\": X, \"y\": y})\n",
        "    ds = ds.batch(batch_size)\n",
        "    # convert tfds into an iterable of dict of NumPy arrays\n",
        "    return tfds.as_numpy(ds)\n",
        "  return get_batches\n",
        "\n",
        "batcher = make_batcher(2, X_train, y_train)\n",
        "\n",
        "for epoch in range(2):\n",
        "  print('epoch {}'.format(epoch))\n",
        "  for batch in batcher():\n",
        "    x, y = batch[\"X\"], batch[\"y\"]\n",
        "    #print(x.shape)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0\n",
            "epoch 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wG9tVufuMTui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sgd(params, loss_fn, grad_loss_fn, get_batches_as_dict, max_epochs, lr):\n",
        "    print_every = max(1, int(0.1*max_epochs))\n",
        "    for epoch in range(max_epochs):\n",
        "        epoch_loss = 0.0\n",
        "        for batch_dict in get_batches_as_dict():\n",
        "            x, y = batch_dict[\"X\"], batch_dict[\"y\"]\n",
        "            batch = (x, y)\n",
        "            batch_grad = grad_loss_fn(params, batch)\n",
        "            params = params - lr*batch_grad\n",
        "            batch_loss = loss_fn(params, batch) # Average loss within this batch\n",
        "            epoch_loss += batch_loss\n",
        "        if epoch % print_every == 0:\n",
        "            print('Epoch {}, Loss {}'.format(epoch, epoch_loss))\n",
        "    return params,\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sV3NbjvM6ai",
        "colab_type": "code",
        "outputId": "e9936b39-9279-4747-f542-c9e166ad13ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "set_seed(0)\n",
        "D = X_train.shape[1]\n",
        "w_init = onp.random.randn(D)\n",
        "\n",
        "def training_loss2(w):\n",
        "    l = NLL(w, (X_train, y_train))\n",
        "    return onp.float64(l)\n",
        "\n",
        "def training_grad2(w):\n",
        "    g = NLL_grad(w, (X_train, y_train))\n",
        "    return onp.asarray(g, dtype=onp.float64)\n",
        "\n",
        "max_epochs = 5\n",
        "lr = 0.1\n",
        "batch_size = 10\n",
        "batcher = make_batcher(batch_size, X_train, y_train)\n",
        "w_mle_sgd = sgd(w_init, NLL, NLL_grad, batcher, max_epochs, lr)\n",
        "print(w_mle_sgd)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss 21.775604248046875\n",
            "Epoch 1, Loss 3.2622179985046387\n",
            "Epoch 2, Loss 3.1074540615081787\n",
            "Epoch 3, Loss 2.9816956520080566\n",
            "Epoch 4, Loss 2.875518798828125\n",
            "(DeviceArray([-0.399, -0.919,  0.311,  2.174], dtype=float32),)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtFGH_OeZUVj",
        "colab_type": "text"
      },
      "source": [
        "## Jax version <a class=\"anchor\" id=\"SGD-jax\"></a>\n",
        "\n",
        "JAX has a small optimization library focused on stochastic first-order optimizers. Every optimizer is modeled as an (`init_fun`, `update_fun`, `get_params`) triple of functions. The `init_fun` is used to initialize the optimizer state, which could include things like momentum variables, and the `update_fun` accepts a gradient and an optimizer state to produce a new optimizer state. The `get_params` function extracts the current iterate (i.e. the current parameters) from the optimizer state. The parameters being optimized can be ndarrays or arbitrarily-nested list/tuple/dict structures, so you can store your parameters however youâ€™d like.\n",
        "\n",
        "Below we show how to reproduce our numpy code using this library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtBbjnzRM79T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Version that uses JAX optimization library\n",
        "\n",
        "\n",
        "#@jit\n",
        "def sgd_jax(params, loss_fn, get_batches, max_epochs, opt_init, opt_update, get_params):\n",
        "    loss_history = []\n",
        "    opt_state = opt_init(params)\n",
        "    \n",
        "    #@jit\n",
        "    def update(i, opt_state, batch):\n",
        "        params = get_params(opt_state)\n",
        "        g = grad(loss_fn)(params, batch)\n",
        "        return opt_update(i, g, opt_state) \n",
        "    \n",
        "    print_every = max(1, int(0.1*max_epochs))\n",
        "    total_steps = 0\n",
        "    for epoch in range(max_epochs):\n",
        "        epoch_loss = 0.0\n",
        "        for batch_dict in get_batches():\n",
        "            X, y = batch_dict[\"X\"], batch_dict[\"y\"]\n",
        "            batch = (X, y)\n",
        "            total_steps += 1\n",
        "            opt_state = update(total_steps, opt_state, batch)\n",
        "        params = get_params(opt_state)\n",
        "        train_loss = onp.float(loss_fn(params, batch))\n",
        "        loss_history.append(train_loss)\n",
        "        if epoch % print_every == 0:\n",
        "            print('Epoch {}, train NLL {}'.format(epoch, train_loss))\n",
        "    return params, loss_history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So3mW_A-cr0n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b=list(batcher())\n",
        "X, y = b[0][\"X\"], b[0][\"y\"]\n",
        "X.shape\n",
        "batch = (X, y)\n",
        "params= w_init\n",
        "onp.float(NLL(params, batch))\n",
        "g = grad(NLL)(params, batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCOrHGTvbbfC",
        "colab_type": "code",
        "outputId": "01273f6a-5f19-4444-d8c0-b4edee7a3d92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# JAX with constant LR should match our minimal version of SGD\n",
        "\n",
        "\n",
        "schedule = optimizers.constant(step_size=lr)\n",
        "opt_init, opt_update, get_params = optimizers.sgd(step_size=schedule)\n",
        "\n",
        "w_mle_sgd2, history = sgd_jax(w_init, NLL, batcher, max_epochs, \n",
        "                              opt_init, opt_update, get_params)\n",
        "print(w_mle_sgd2)\n",
        "print(history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, train NLL 0.3694833219051361\n",
            "Epoch 1, train NLL 0.3485594689846039\n",
            "Epoch 2, train NLL 0.33153384923934937\n",
            "Epoch 3, train NLL 0.31704843044281006\n",
            "Epoch 4, train NLL 0.3043736517429352\n",
            "[-0.399 -0.919  0.311  2.174]\n",
            "[0.3694833219051361, 0.3485594689846039, 0.33153384923934937, 0.31704843044281006, 0.3043736517429352]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHI0RPrPblpY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}