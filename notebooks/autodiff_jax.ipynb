{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "autodiff_jax.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "DeXQ1pOcmhKu",
        "D2dSgG-cnVIe",
        "riXmT3Iok_yq",
        "QuMHSd3wr1xH",
        "kUj-VuSzmFaV",
        "VTIybB8b4ar0",
        "nglie5m7q607",
        "bBMcsg4uoKua",
        "mg9ValMRm_Md",
        "MeoGcnV54YY9",
        "EOzQJSX3JOF9",
        "ZF_OjIRrisDB",
        "AKTg9m9yz7Ib",
        "ZMZVMEOPbLWt",
        "TdtqCDWZAC2f",
        "A-L-dB_GBWNq",
        "YFsfzYHzhbM3",
        "QGxWcHWJKtsq",
        "o8iMrvUXK8Id",
        "8-wtZiZmU329",
        "Ru5D0tZEiV2e"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/probml/pyprobml/blob/master/book1/supplements/autodiff_jax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4bE-S8yDALH"
      },
      "source": [
        "# Automatic differentiation using JAX \n",
        "\n",
        "In this section, we illustrate automatic differentation using JAX.\n",
        "For details, see see  [this video](https://www.youtube.com/watch?v=wG_nF1awSSY&t=697s)  or [The Autodiff Cookbook](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCI0G3tfDFSs"
      },
      "source": [
        "# Standard Python libraries\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "from functools import partial\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "np.set_printoptions(precision=3)\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "import imageio\n",
        "\n",
        "from typing import Tuple, NamedTuple\n",
        "\n",
        "from IPython import display\n",
        "%matplotlib inline\n",
        "\n",
        "import sklearn\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9kAsUWYDIOk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4568e80c-e8a5-4a81-ecb0-20a2d994b0b1"
      },
      "source": [
        "\n",
        "# Load JAX\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "from jax import random, vmap, jit, grad, value_and_grad, hessian, jacfwd, jacrev\n",
        "print(\"jax version {}\".format(jax.__version__))\n",
        "# Check the jax backend\n",
        "print(\"jax backend {}\".format(jax.lib.xla_bridge.get_backend().platform))\n",
        "key = random.PRNGKey(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jax version 0.2.9\n",
            "jax backend gpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuMHSd3wr1xH"
      },
      "source": [
        "## Derivatives\n",
        "\n",
        "We can compute $(\\nabla f)(x)$ using `grad(f)(x)`. For example, consider\n",
        "\n",
        "\n",
        "$f(x) = x^3 + 2x^2 - 3x + 1$\n",
        "\n",
        "$f'(x) = 3x^2 + 4x -3$\n",
        "\n",
        "$f''(x) = 6x + 4$\n",
        "\n",
        "$f'''(x) = 6$\n",
        "\n",
        "$f^{iv}(x) = 0$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYTM5MPmr3C0",
        "outputId": "4801ef39-5c62-4fd1-c021-dc2d23e03035"
      },
      "source": [
        "f = lambda x: x**3 + 2*x**2 - 3*x + 1\n",
        "\n",
        "dfdx = jax.grad(f)\n",
        "d2fdx = jax.grad(dfdx)\n",
        "d3fdx = jax.grad(d2fdx)\n",
        "d4fdx = jax.grad(d3fdx)\n",
        "\n",
        "print(dfdx(1.))\n",
        "print(d2fdx(1.))\n",
        "print(d3fdx(1.))\n",
        "print(d4fdx(1.))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.0\n",
            "10.0\n",
            "6.0\n",
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUj-VuSzmFaV"
      },
      "source": [
        "## Partial derivatives\n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "f(x,y) &= x^2 + y \\\\\n",
        "\\frac{\\partial f}{\\partial x} &= 2x \\\\\n",
        "\\frac{\\partial f}{\\partial y} &= 1 \n",
        "\\end{align}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0hW7fqfmR1c",
        "outputId": "3b5bbc43-a7e3-4692-c4e8-137faa0c68eb"
      },
      "source": [
        "def f(x,y):\n",
        "  return x**2 + y\n",
        "\n",
        "# Partial derviatives\n",
        "x = 2.0; y= 3.0;\n",
        "v, gx = value_and_grad(f, argnums=0)(x,y)\n",
        "print(v)\n",
        "print(gx)\n",
        "\n",
        "gy = grad(f, argnums=1)(x,y)\n",
        "print(gy)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7.0\n",
            "4.0\n",
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTIybB8b4ar0"
      },
      "source": [
        "## Gradients "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xb0gZ_1HBEyC"
      },
      "source": [
        "Linear function: multi-input, scalar output.\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "f(x; a) &= a^T x\\\\\n",
        "\\nabla_x f(x;a) &= a\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmYqFEs04vkV",
        "outputId": "311e8ac4-3ed4-4ad6-f545-6307390d4745"
      },
      "source": [
        "\n",
        "\n",
        "def fun1d(x):\n",
        "    return jnp.dot(a, x)[0]\n",
        "\n",
        "Din = 3; Dout = 1;\n",
        "a = np.random.normal(size=(Dout, Din))\n",
        "x = np.random.normal(size=(Din,))\n",
        "\n",
        "g = grad(fun1d)(x)\n",
        "assert np.allclose(g, a)\n",
        "\n",
        "\n",
        "# It is often useful to get the function value and gradient at the same time\n",
        "val_grad_fn = jax.value_and_grad(fun1d)\n",
        "v, g = val_grad_fn(x)\n",
        "print(v)\n",
        "print(g)\n",
        "assert np.allclose(v, fun1d(x))\n",
        "assert np.allclose(a, g)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-1.0599848\n",
            "[-1.311  0.546  0.915]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbgiqkF6BL1E"
      },
      "source": [
        "Linear function: multi-input, multi-output.\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "f(x;A) &= A x \\\\\n",
        "\\frac{\\partial f(x;A)}{\\partial x} &= A\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6hkEYxV5EIx"
      },
      "source": [
        "# We construct a multi-output linear function.\n",
        "# We check forward and reverse mode give same Jacobians.\n",
        "\n",
        "\n",
        "def fun(x):\n",
        "    return jnp.dot(A, x)\n",
        "\n",
        "Din = 3; Dout = 4;\n",
        "A = np.random.normal(size=(Dout, Din))\n",
        "x = np.random.normal(size=(Din,))\n",
        "Jf = jacfwd(fun)(x)\n",
        "Jr = jacrev(fun)(x)\n",
        "assert np.allclose(Jf, Jr)\n",
        "assert np.allclose(Jf, A)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CN5d-D7XBU9Y"
      },
      "source": [
        "Quadratic form.\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "f(x;A) &= x^T A x \\\\\n",
        "\\nabla_x f(x;A) &= (A+A^T) x\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9URZeX8PBbhl"
      },
      "source": [
        "\n",
        "D = 4\n",
        "A = np.random.normal(size=(D,D))\n",
        "x = np.random.normal(size=(D,))\n",
        "quadfun = lambda x: jnp.dot(x, jnp.dot(A, x))\n",
        "\n",
        "g = grad(quadfun)(x)\n",
        "assert np.allclose(g, jnp.dot(A+A.T, x))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9ZOhDeqCXu3"
      },
      "source": [
        "Chain rule applied to sigmoid function.\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\mu(x;w) &=\\sigma(w^T x) \\\\\n",
        "\\nabla_w \\mu(x;w) &= \\sigma'(w^T x) x \\\\\n",
        "\\sigma'(a) &= \\sigma(a) * (1-\\sigma(a)) \n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6q5VfLXLB7rv",
        "outputId": "0773a46f-784f-4dbe-a6b7-29dd5cd26654"
      },
      "source": [
        "\n",
        "\n",
        "D = 4\n",
        "w = np.random.normal(size=(D,))\n",
        "x = np.random.normal(size=(D,))\n",
        "y = 0 \n",
        "\n",
        "def sigmoid(x): return 0.5 * (jnp.tanh(x / 2.) + 1)\n",
        "def mu(w): return sigmoid(jnp.dot(w,x))\n",
        "def deriv_mu(w): return mu(w) * (1-mu(w)) * x\n",
        "deriv_mu_jax =  grad(mu)\n",
        "\n",
        "print(deriv_mu(w))\n",
        "print(deriv_mu_jax(w))\n",
        "\n",
        "assert np.allclose(deriv_mu(w), deriv_mu_jax(w), atol=1e-3)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.458  0.022 -0.266 -0.005]\n",
            "[-0.458  0.022 -0.266 -0.005]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nglie5m7q607"
      },
      "source": [
        "## Auxiliary return values\n",
        "\n",
        "A function can return its value and other auxiliary results; the latter are not differentiated. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHz6zrC9qVjT",
        "outputId": "76724fd1-4e1a-4737-b252-963700fcce91"
      },
      "source": [
        "def f(x,y):\n",
        "  return x**2+y, 42\n",
        "\n",
        "(v,aux), g = value_and_grad(f, has_aux=True)(x,y)\n",
        "print(v)\n",
        "print(aux)\n",
        "print(g)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7.0\n",
            "42\n",
            "4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBMcsg4uoKua"
      },
      "source": [
        "## Jacobians\n",
        "\n",
        "\n",
        "Example: Linear function: multi-input, multi-output.\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "f(x;A) &= A x \\\\\n",
        "\\frac{\\partial f(x;A)}{\\partial x} &= A\n",
        "\\end{align}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPilm5H3oWcy"
      },
      "source": [
        "# We construct a multi-output linear function.\n",
        "# We check forward and reverse mode give same Jacobians.\n",
        "\n",
        "\n",
        "def fun(x):\n",
        "    return jnp.dot(A, x)\n",
        "\n",
        "Din = 3; Dout = 4;\n",
        "A = np.random.normal(size=(Dout, Din))\n",
        "x = np.random.normal(size=(Din,))\n",
        "Jf = jacfwd(fun)(x)\n",
        "Jr = jacrev(fun)(x)\n",
        "assert np.allclose(Jf, Jr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg9ValMRm_Md"
      },
      "source": [
        "## Hessians\n",
        "\n",
        "Quadratic form.\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "f(x;A) &= x^T A x \\\\\n",
        "\\nabla_x^2 f(x;A) &= A + A^T\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leW9lqvinDsM"
      },
      "source": [
        "\n",
        "D = 4\n",
        "A = np.random.normal(size=(D,D))\n",
        "x = np.random.normal(size=(D,))\n",
        "\n",
        "quadfun = lambda x: jnp.dot(x, jnp.dot(A, x))\n",
        "\n",
        "\n",
        "H1 = hessian(quadfun)(x)\n",
        "assert np.allclose(H1, A+A.T)\n",
        "\n",
        "def my_hessian(fun):\n",
        "  return jacfwd(jacrev(fun))\n",
        "\n",
        "H2 = my_hessian(quadfun)(x)\n",
        "assert np.allclose(H1, H2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeoGcnV54YY9"
      },
      "source": [
        "## Example: Binary logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Isql2l4MGfIt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2465d977-66dc-4ea0-e5bd-5a05d4ad92ec"
      },
      "source": [
        "\n",
        "def sigmoid(x): return 0.5 * (jnp.tanh(x / 2.) + 1)\n",
        "\n",
        "def predict_single(w, x):\n",
        "    return sigmoid(jnp.dot(w, x)) # <(D) , (D)> = (1) # inner product\n",
        "  \n",
        "def predict_batch(w, X):\n",
        "    return sigmoid(jnp.dot(X, w)) # (N,D) * (D,1) = (N,1) # matrix-vector multiply\n",
        "\n",
        "# negative log likelihood\n",
        "def loss(weights, inputs, targets):\n",
        "    preds = predict_batch(weights, inputs)\n",
        "    logprobs = jnp.log(preds) * targets + jnp.log(1 - preds) * (1 - targets)\n",
        "    return -jnp.sum(logprobs)\n",
        "\n",
        "\n",
        "D = 2\n",
        "N = 3\n",
        "w = jax.random.normal(key, shape=(D,))\n",
        "X = jax.random.normal(key, shape=(N,D))\n",
        "y = jax.random.choice(key, 2, shape=(N,)) # uniform binary labels\n",
        "#logits = jnp.dot(X, w)\n",
        "#y = jax.random.categorical(key, logits)\n",
        "\n",
        "print(loss(w, X, y))\n",
        "\n",
        "# Gradient function\n",
        "grad_fun = grad(loss)\n",
        "\n",
        "# Gradient of each example in the batch - 2 different ways\n",
        "grad_fun_w = partial(grad_fun, w)\n",
        "grads = vmap(grad_fun_w)(X,y)\n",
        "print(grads)\n",
        "assert grads.shape == (N,D)\n",
        "\n",
        "grads2 = vmap(grad_fun, in_axes=(None, 0, 0))(w, X, y) \n",
        "assert np.allclose(grads, grads2)\n",
        "\n",
        "# Gradient for entire batch\n",
        "grad_sum = jnp.sum(grads, axis=0)\n",
        "assert grad_sum.shape == (D,)\n",
        "print(grad_sum)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.5545294\n",
            "[[ 0.042 -0.287]\n",
            " [-0.236 -0.454]\n",
            " [-0.14   0.067]]\n",
            "[-0.334 -0.673]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3BaHdT4Gj6W",
        "outputId": "12dc846e-dd7e-4907-ea76-73147ea6f77f"
      },
      "source": [
        "# Textbook implementation of gradient\n",
        "def NLL_grad(weights, batch):\n",
        "    X, y = batch\n",
        "    N = X.shape[0]\n",
        "    mu = predict_batch(weights, X)\n",
        "    g = jnp.sum(jnp.dot(jnp.diag(mu - y), X), axis=0)\n",
        "    return g\n",
        "\n",
        "grad_sum_batch = NLL_grad(w, (X,y))\n",
        "print(grad_sum_batch)\n",
        "assert np.allclose(grad_sum, grad_sum_batch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.334 -0.673]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_4lRrHgpLbG",
        "outputId": "d47db27a-b171-4933-de08-58bc5bbe0c2f"
      },
      "source": [
        "# We can also compute Hessians, as we illustrate below.\n",
        "\n",
        "hessian_fun = hessian(loss)\n",
        "\n",
        "# Hessian on one example\n",
        "H0 = hessian_fun(w, X[0,:], y[0])\n",
        "print('Hessian(example 0)\\n{}'.format(H0))\n",
        "\n",
        "# Hessian for batch\n",
        "Hbatch = vmap(hessian_fun, in_axes=(None, 0, 0))(w, X, y) \n",
        "print('Hbatch shape {}'.format(Hbatch.shape))\n",
        "\n",
        "Hbatch_sum = jnp.sum(Hbatch, axis=0)\n",
        "print('Hbatch sum\\n {}'.format(Hbatch_sum))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hessian(example 0)\n",
            "[[ 0.006 -0.042]\n",
            " [-0.042  0.286]]\n",
            "Hbatch shape (3, 2, 2)\n",
            "Hbatch sum\n",
            " [[0.118 0.139]\n",
            " [0.139 0.65 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcJvgukUpWWE"
      },
      "source": [
        "# Textbook implementation of Hessian\n",
        "\n",
        "def NLL_hessian(weights, batch):\n",
        "  X, y = batch\n",
        "  mu = predict_batch(weights, X)\n",
        "  S = jnp.diag(mu * (1-mu))\n",
        "  H = jnp.dot(jnp.dot(X.T, S), X)\n",
        "  return H\n",
        "\n",
        "H2 = NLL_hessian(w, (X,y) )\n",
        "\n",
        "assert np.allclose(Hbatch_sum, H2, atol=1e-2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOzQJSX3JOF9"
      },
      "source": [
        "## Vector Jacobian Products (VJP) and Jacobian Vector Products (JVP)\n",
        "\n",
        "Consider a bilinear mapping $f(x,W) = x W$.\n",
        "For fixed parameters, we have\n",
        "$f1(x) = W x$, so $J(x) = W$, and $u^T J(x) = J(x)^T u = W^T u$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Lpmntn2JREG",
        "outputId": "39778872-aca9-488c-f9dc-40d30a821488"
      },
      "source": [
        "n = 3; m = 2;\n",
        "W = jax.random.normal(key, shape=(m,n))\n",
        "x = jax.random.normal(key, shape=(n,))\n",
        "u = jax.random.normal(key, shape=(m,))\n",
        "\n",
        "def f1(x): return jnp.dot(W,x)\n",
        "\n",
        "J1 = jacfwd(f1)(x)\n",
        "print(J1.shape)\n",
        "\n",
        "assert np.allclose(J1, W)\n",
        "tmp1 = jnp.dot(u.T, J1)\n",
        "print(tmp1)\n",
        "\n",
        "(val, jvp_fun) = jax.vjp(f1, x)\n",
        "\n",
        "tmp2 = jvp_fun(u)\n",
        "\n",
        "assert np.allclose(tmp1, tmp2)\n",
        "\n",
        "tmp3 = np.dot(W.T, u)\n",
        "assert np.allclose(tmp1, tmp3)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 3)\n",
            "[ 0.922  1.216 -0.61 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYSC6DMOO3IS"
      },
      "source": [
        "For fixed inputs, we have\n",
        "$f2(W) = W x$, so $J(W) = \\text{something complex}$,\n",
        "but $u^T J(W) = J(W)^T u = u x^T$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8l3StJdO1r1",
        "outputId": "5a3f1e5b-b903-4db8-bb8a-a53c29bcad19"
      },
      "source": [
        "\n",
        "def f2(W): return jnp.dot(W,x)\n",
        "\n",
        "J2 = jacfwd(f2)(W)\n",
        "print(J2.shape)\n",
        "\n",
        "tmp1 = jnp.dot(u.T, J2)\n",
        "print(tmp1)\n",
        "print(tmp1.shape)\n",
        "\n",
        "(val, jvp_fun) = jax.vjp(f2, W)\n",
        "tmp2 = jvp_fun(u)\n",
        "assert np.allclose(tmp1, tmp2)\n",
        "\n",
        "tmp3 = np.outer(u, x)\n",
        "assert np.allclose(tmp1, tmp3)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 2, 3)\n",
            "[[-1.425  0.379 -0.267]\n",
            " [ 1.555 -0.413  0.291]]\n",
            "(2, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF_OjIRrisDB"
      },
      "source": [
        "## Stop-gradient\n",
        "\n",
        "Sometimes we want to take the gradient of a complex expression wrt some parameters $\\theta$, but treating $\\theta$ as a constant for some parts of the expression. For example, consider the TD(0) update in reinforcement learning, which as the following form:\n",
        "\n",
        "\n",
        "$\\Delta \\theta = (r_t + v_{\\theta}(s_t) - v_{\\theta}(s_{t-1})) \\nabla v_{\\theta}(s_{t-1})$\n",
        "\n",
        "where $s$ is the state, $r$ is the reward, and $v$ is the value function.\n",
        "This update is not the gradient of any loss function.\n",
        "However it can be **written** as the gradient of the pseudo loss function\n",
        "\n",
        "$L(\\theta) = [r_t + v_{\\theta}(s_t) - v_{\\theta}(s_{t-1})]^2$\n",
        "\n",
        "since\n",
        "\n",
        "$\\nabla_{\\theta} L(\\theta) = 2 [r_t + v_{\\theta}(s_t) - v_{\\theta}(s_{t-1})] \\nabla v_{\\theta}(s_{t-1})$\n",
        "\n",
        "if the dependency of the target $r_t + v_{\\theta}(s_t)$ on the parameter $\\theta$ is ignored. We can implement this in JAX using `stop_gradient`, as we show below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9cprxb9jbsK",
        "outputId": "b345e287-b8fa-417f-d20e-0f1951e1f64e"
      },
      "source": [
        "def td_loss(theta, s_prev, r_t, s_t):\n",
        "  v_prev = value_fn(theta, s_prev)\n",
        "  target = r_t + value_fn(theta, s_t)\n",
        "  return 0.5*(jax.lax.stop_gradient(target) - v_prev) ** 2\n",
        "\n",
        "td_update = jax.grad(td_loss)\n",
        "\n",
        "# An example transition.\n",
        "s_prev = jnp.array([1., 2., -1.])\n",
        "r_t = jnp.array(1.)\n",
        "s_t = jnp.array([2., 1., 0.])\n",
        "\n",
        "# Value function and initial parameters\n",
        "value_fn = lambda theta, state: jnp.dot(theta, state)\n",
        "theta = jnp.array([0.1, -0.1, 0.])\n",
        "\n",
        "print(td_update(theta, s_prev, r_t, s_t))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-1.2 -2.4  1.2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKTg9m9yz7Ib"
      },
      "source": [
        "## Straight through estimator\n",
        "\n",
        "The straight-through estimator is a trick for defining a 'gradient' of a function that is otherwise non-differentiable. Given a non-differentiable function $f : \\mathbb{R}^n \\to \\mathbb{R}^n$ that is used as part of a larger function that we wish to find a gradient of, we simply pretend during the backward pass that $f$ is the identity function, so gradients pass through $f$ ignoring the $f'$ term. This can be implemented neatly using `jax.lax.stop_gradient`.\n",
        "\n",
        "Here is an example of a non-differentiable function that converts a soft probability distribution to a one-hot vector (discretization).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIFVQKrwqAG4",
        "outputId": "67ada739-47e3-4ef9-c11e-76a64862ea80"
      },
      "source": [
        "def onehot(labels, num_classes):\n",
        "  y = (labels[..., None] == jnp.arange(num_classes)[None])\n",
        "  return y.astype(jnp.float32)\n",
        "\n",
        "def quantize(y_soft): \n",
        "  y_hard = onehot(jnp.argmax(y_soft), 3)[0]\n",
        "  return y_hard\n",
        "\n",
        "y_soft = np.array([0.1, 0.2, 0.7])\n",
        "print(quantize(y_soft))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSuQMr61sg16"
      },
      "source": [
        "Now suppose we define some linear function of the quantized variable of the form $f(y) = w^T q(y)$. If $w=[1,2,3]$ and $q(y)=[0,0,1]$, we get $f(y) = 3$. But the gradient is 0 because $q$ is not differentiable.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDEMPSdJsTpl",
        "outputId": "0f329a87-2271-47d2-f23b-be47ed79a618"
      },
      "source": [
        "def f(y):\n",
        "  w = jnp.array([1,2,3])\n",
        "  yq = quantize(y)\n",
        "  return jnp.dot(w, yq)\n",
        "\n",
        "print(f(y_soft))\n",
        "print(grad(f)(y_soft))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.0\n",
            "[0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTcVC08Rs4DM"
      },
      "source": [
        "To use the straight-through estimator, we replace $q(y)$ with \n",
        "$$y + SG(q(y)-y)$$, where SG is stop gradient. In the forwards pass, we have $y+q(y)-y=q(y)$. In the backwards pass, the gradient of SG is 0, so we effectively replace $q(y)$ with $y$. So in the backwarsd pass we have\n",
        "$$\n",
        "\\begin{align}\n",
        "f(y) &= w^T q(y) \\approx w^T  y \\\\\n",
        "\\nabla_y f(y) &\\approx w\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5m_k7Ju3sUVq",
        "outputId": "cfcd7769-de40-4fc7-d29c-19d6a7d04db0"
      },
      "source": [
        "\n",
        "\n",
        "def f_ste(y):\n",
        "  w = jnp.array([1,2,3])\n",
        "  yq = quantize(y)\n",
        "  yy = y + jax.lax.stop_gradient(yq - y) # gives yq on fwd, and y on backward\n",
        "  return jnp.dot(w, yy)\n",
        "\n",
        "print(f_ste(y_soft))\n",
        "print(grad(f_ste)(y_soft))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.0\n",
            "[1. 2. 3.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwMIYqiOp0U0"
      },
      "source": [
        "## Per-example gradients\n",
        "\n",
        "In some applications, we want to compute the gradient for every example in a batch, not just the sum of gradients over the batch. This is hard in other frameworks like TF and PyTorch but easy in JAX, as we show below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKhNPiq4qGhl",
        "outputId": "0102b6ff-1fc1-4870-9f58-1b4e7db92ba9"
      },
      "source": [
        "def loss(w, x):\n",
        "  return jnp.dot(w,x)\n",
        "\n",
        "w = jnp.ones((3,))\n",
        "x0 = jnp.array([1.0, 2.0, 3.0])\n",
        "x1 = 2*x0\n",
        "X = jnp.stack([x0, x1])\n",
        "print(X.shape)\n",
        "\n",
        "perex_grads = jax.jit(jax.vmap(jax.grad(loss), in_axes=(None, 0)))\n",
        "print(perex_grads(w, X))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 3)\n",
            "[[1. 2. 3.]\n",
            " [2. 4. 6.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxPeXpUaq-_p"
      },
      "source": [
        "To explain the above code in more depth, note that the vmap converts the function loss to take  a batch of inputs for each of its arguments, and returns a batch of outputs. To make it work with a single weight vector, we specify in_axes=(None,0), meaning the first argument (w) is not replicated, and the second argument (x) is replicated along dimension 0. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYB1L8JKrVKo",
        "outputId": "b78026f2-7ff2-4228-be72-cd0c1b281b2b"
      },
      "source": [
        "gradfn = jax.grad(loss)\n",
        "\n",
        "W = jnp.stack([w, w])\n",
        "print(jax.vmap(gradfn)(W, X))\n",
        "\n",
        "print(jax.vmap(gradfn, in_axes=(None,0))(w, X))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 2. 3.]\n",
            " [2. 4. 6.]]\n",
            "[[1. 2. 3.]\n",
            " [2. 4. 6.]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}