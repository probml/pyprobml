{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000db3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online training of a logistic regression model\n",
    "# using Assumed Density Filtering (ADF).\n",
    "# We compare the ADF result with MCMC sampling\n",
    "# For further details, see the ADF paper:\n",
    "#   * O. Zoeter, \"Bayesian Generalized Linear Models in a Terabyte World,\"\n",
    "#     2007 5th International Symposium on Image and Signal Processing and Analysis, 2007,\n",
    "#     pp. 435-440, doi: 10.1109/ISPA.2007.4383733.\n",
    "# of the posterior distribution\n",
    "# Dependencies:\n",
    "#   !pip install -qq jax_cosmo\n",
    "\n",
    "# Author: Gerardo Durán-Martín (@gerdm)\n",
    "\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import probml_utils as pml\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -qq git+https://github.com/probml/probml-utils.git\n",
    "    import probml_utils as pml\n",
    "from jax import random\n",
    "from jax.scipy.stats import norm\n",
    "\n",
    "try:\n",
    "    from jax_cosmo.scipy import integrate\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -qq jax_cosmo\n",
    "    from jax_cosmo.scipy import integrate\n",
    "from functools import partial\n",
    "\n",
    "try:\n",
    "    from jsl.demos import logreg_biclusters_demo as demo\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -qq jsl\n",
    "    from jsl.demos import logreg_biclusters_demo as demo\n",
    "\n",
    "import probml_utils as pml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9483698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosmo seems to only support numerical integration in CPU mode\n",
    "jax.config.update(\"jax_platform_name\", \"cpu\")\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "figures, data = demo.main()\n",
    "\n",
    "X = data[\"X\"]\n",
    "y = data[\"y\"]\n",
    "Phi = data[\"Phi\"]\n",
    "Xspace = data[\"Xspace\"]\n",
    "Phispace = data[\"Phispace\"]\n",
    "w_laplace = data[\"w_laplace\"]\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return jnp.exp(z) / (1 + jnp.exp(z))\n",
    "\n",
    "\n",
    "def log_sigmoid(z):\n",
    "    return z - jnp.log1p(jnp.exp(z))\n",
    "\n",
    "\n",
    "def Zt_func(eta, y, mu, v):\n",
    "    log_term = y * log_sigmoid(eta) + (1 - y) * jnp.log1p(-sigmoid(eta))\n",
    "    log_term = log_term + norm.logpdf(eta, mu, v)\n",
    "\n",
    "    return jnp.exp(log_term)\n",
    "\n",
    "\n",
    "def mt_func(eta, y, mu, v, Zt):\n",
    "    log_term = y * log_sigmoid(eta) + (1 - y) * jnp.log1p(-sigmoid(eta))\n",
    "    log_term = log_term + norm.logpdf(eta, mu, v)\n",
    "\n",
    "    return eta * jnp.exp(log_term) / Zt\n",
    "\n",
    "\n",
    "def vt_func(eta, y, mu, v, Zt):\n",
    "    log_term = y * log_sigmoid(eta) + (1 - y) * jnp.log1p(-sigmoid(eta))\n",
    "    log_term = log_term + norm.logpdf(eta, mu, v)\n",
    "\n",
    "    return eta**2 * jnp.exp(log_term) / Zt\n",
    "\n",
    "\n",
    "def adf_step(state, xs, prior_variance, lbound, ubound):\n",
    "    mu_t, tau_t = state\n",
    "    Phi_t, y_t = xs\n",
    "\n",
    "    mu_t_cond = mu_t\n",
    "    tau_t_cond = tau_t + prior_variance\n",
    "\n",
    "    # prior predictive distribution\n",
    "    m_t_cond = (Phi_t * mu_t_cond).sum()\n",
    "    v_t_cond = (Phi_t**2 * tau_t_cond).sum()\n",
    "\n",
    "    v_t_cond_sqrt = jnp.sqrt(v_t_cond)\n",
    "\n",
    "    # Moment-matched Gaussian approximation elements\n",
    "    Zt = integrate.romb(lambda eta: Zt_func(eta, y_t, m_t_cond, v_t_cond_sqrt), lbound, ubound)\n",
    "\n",
    "    mt = integrate.romb(lambda eta: mt_func(eta, y_t, m_t_cond, v_t_cond_sqrt, Zt), lbound, ubound)\n",
    "\n",
    "    vt = integrate.romb(lambda eta: vt_func(eta, y_t, m_t_cond, v_t_cond_sqrt, Zt), lbound, ubound)\n",
    "    vt = vt - mt**2\n",
    "\n",
    "    # Posterior estimation\n",
    "    delta_m = mt - m_t_cond\n",
    "    delta_v = vt - v_t_cond\n",
    "    a = Phi_t * tau_t_cond / (Phi_t**2 * tau_t_cond).sum()\n",
    "    mu_t = mu_t_cond + a * delta_m\n",
    "    tau_t = tau_t_cond + a**2 * delta_v\n",
    "\n",
    "    return (mu_t, tau_t), (mu_t, tau_t)\n",
    "\n",
    "\n",
    "# ** ADF inference **\n",
    "prior_variance = 0.0\n",
    "# Lower and upper bounds of integration. Ideally, we would like to\n",
    "# integrate from -inf to inf, but we run into numerical issues.\n",
    "n_datapoints, ndims = Phi.shape\n",
    "lbound, ubound = -20, 20\n",
    "mu_t = jnp.zeros(ndims)\n",
    "tau_t = jnp.ones(ndims) * 1.0\n",
    "\n",
    "init_state = (mu_t, tau_t)\n",
    "xs = (Phi, y)\n",
    "\n",
    "adf_loop = partial(adf_step, prior_variance=prior_variance, lbound=lbound, ubound=ubound)\n",
    "(mu_t, tau_t), (mu_t_hist, tau_t_hist) = jax.lax.scan(adf_loop, init_state, xs)\n",
    "print(\"ADF weights\")\n",
    "print(mu_t)\n",
    "\n",
    "# ADF posterior predictive distribution\n",
    "n_samples = 5000\n",
    "key = random.PRNGKey(3141)\n",
    "adf_samples = random.multivariate_normal(key, mu_t, jnp.diag(tau_t), (n_samples,))\n",
    "Z_adf = sigmoid(jnp.einsum(\"mij,sm->sij\", Phispace, adf_samples))\n",
    "Z_adf = Z_adf.mean(axis=0)\n",
    "\n",
    "# ** Plotting predictive distribution **\n",
    "colors = [\"black\" if el else \"white\" for el in y]\n",
    "\n",
    "## Add posterior marginal for ADF-estimated weights\n",
    "for i in range(ndims):\n",
    "    mean, std = mu_t[i], jnp.sqrt(tau_t[i])\n",
    "    # fig = figures[f\"weights_marginals_w{i}\"]\n",
    "    fig = figures[f\"logistic_regression_weights_marginals_w{i}\"]\n",
    "    ax = fig.gca()\n",
    "    x = jnp.linspace(mean - 4 * std, mean + 4 * std, 500)\n",
    "    ax.plot(x, norm.pdf(x, mean, std), label=\"posterior (ADF)\", linestyle=\"dashdot\")\n",
    "    ax.legend()\n",
    "\n",
    "fig_adf, ax = plt.subplots()\n",
    "title = \"ADF Predictive distribution\"\n",
    "demo.plot_posterior_predictive(ax, X, Xspace, Z_adf, title, colors)\n",
    "# figures[\"predictive_distribution_adf\"] = fig_adf\n",
    "# figures[\"logistic_regression_surface_adf\"] = fig_adf\n",
    "pml.savefig(\"logistic_regression_surface_adf.pdf\")\n",
    "\n",
    "# Posterior vs time\n",
    "\n",
    "lcolors = [\"black\", \"tab:blue\", \"tab:red\"]\n",
    "elements = mu_t_hist.T, tau_t_hist.T, w_laplace, lcolors\n",
    "timesteps = jnp.arange(n_datapoints) + 1\n",
    "\n",
    "for k, (wk, Pk, wk_laplace, c) in enumerate(zip(*elements)):\n",
    "    fig_weight_k, ax = plt.subplots()\n",
    "    ax.errorbar(timesteps, wk, jnp.sqrt(Pk), c=c, label=f\"$w_{k}$ online (adf)\")\n",
    "    ax.axhline(y=wk_laplace, c=c, linestyle=\"dotted\", label=f\"$w_{k}$ batch (Laplace)\", linewidth=3)\n",
    "\n",
    "    ax.set_xlim(1, n_datapoints)\n",
    "    ax.legend(framealpha=0.7, loc=\"upper right\")\n",
    "    ax.set_xlabel(\"number samples\")\n",
    "    ax.set_ylabel(\"weights\")\n",
    "    plt.tight_layout()\n",
    "    # figures[f\"adf_logistic_regression_hist_w{k}\"] = fig_weight_k\n",
    "    # figures[f\"logistic_regression_hist_adf_w{k}\"] = fig_weight_k\n",
    "    pml.savefig(f\"logistic_regression_hist_adf_w{k}\")\n",
    "\n",
    "# for name, figure in figures.items():\n",
    "#    filename = f\"./../figures/{name}.pdf\"\n",
    "#    figure.savefig(filename)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
