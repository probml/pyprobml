{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "logreg_jax.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nirzu97/pyprobml/blob/logreg_jax/notebooks/logreg_jax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eB0c7K3GpBrg"
      },
      "source": [
        "# Logistic regression <a class=\"anchor\" id=\"logreg\"></a>\n",
        "\n",
        "In this notebook, we illustrate how to perform logistic regression on some small datasets. We will compare binary logistic regression as implemented by sklearn with our own implementation, for which we use a batch optimizer from scipy. We code the gradients by hand. We also show how to use the JAX autodiff package (see [JAX AD colab](https://github.com/probml/pyprobml/tree/master/book1/supplements/autodiff_jax.ipynb)).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ml8l4WVLpWCI"
      },
      "source": [
        "# Standard Python libraries\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "import imageio\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "import sklearn\n",
        "\n",
        "import seaborn as sns;\n",
        "sns.set(style=\"ticks\", color_codes=True)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cH-3xvdv7dXD",
        "outputId": "66dc6c29-9c1b-4419-cfb9-da7b1c0bef1d"
      },
      "source": [
        "# https://github.com/google/jax\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax.scipy.special import logsumexp\n",
        "from jax import grad, hessian, jacfwd, jacrev, jit, vmap\n",
        "from jax.experimental import optimizers\n",
        "print(\"jax version {}\".format(jax.__version__))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jax version 0.2.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UrGDEYb58qF"
      },
      "source": [
        "# First we create a dataset.\n",
        "\n",
        "import sklearn.datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "iris = sklearn.datasets.load_iris()\n",
        "X = iris[\"data\"]\n",
        "y = (iris[\"target\"] == 2).astype(np.int)  # 1 if Iris-Virginica, else 0'\n",
        "N, D = X.shape # 150, 4\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8GCOQm16OB3",
        "outputId": "69703c4d-84dd-48ff-dc12-1b9b0680ade1"
      },
      "source": [
        "# Now let's find the MLE using sklearn. We will use this as the \"gold standard\"\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# We set C to a large number to turn off regularization.\n",
        "# We don't fit the bias term to simplify the comparison below.\n",
        "log_reg = LogisticRegression(solver=\"lbfgs\", C=1e5, fit_intercept=False)\n",
        "log_reg.fit(X_train, y_train)\n",
        "w_mle_sklearn = jnp.ravel(log_reg.coef_)\n",
        "print(w_mle_sklearn)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-4.41378437 -9.11061763  6.53872233 12.68572678]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FtE7SSE6V0k",
        "outputId": "80b5de9f-f2a4-46bb-d6f4-777563472401"
      },
      "source": [
        "\n",
        "# First we define the model, and check it gives the same output as sklearn.\n",
        "\n",
        "def sigmoid(x): return 0.5 * (jnp.tanh(x / 2.) + 1)\n",
        "\n",
        "def predict_logit(weights, inputs):\n",
        "    return jnp.dot(inputs, weights) # Already vectorized\n",
        "\n",
        "def predict_prob(weights, inputs):\n",
        "    return sigmoid(predict_logit(weights, inputs))\n",
        "\n",
        "ptest_sklearn = log_reg.predict_proba(X_test)[:,1]\n",
        "print(jnp.round(ptest_sklearn, 3))\n",
        "\n",
        "ptest_us = predict_prob(w_mle_sklearn, X_test)\n",
        "print(jnp.round(ptest_us, 3))\n",
        "\n",
        "assert jnp.allclose(ptest_sklearn, ptest_us, atol=1e-2)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0.002 0.    1.    0.012 0.002 0.    0.    0.979 0.74  0.    0.706 0.\n",
            " 0.    0.    0.    0.001 1.    0.    0.009 1.    0.    0.65  0.    1.\n",
            " 0.094 0.998 1.    1.    0.    0.    0.    0.    0.    0.    0.    0.998\n",
            " 0.    0.    0.    0.    0.999 0.    0.    0.    0.    0.    0.281 0.909\n",
            " 0.    0.999]\n",
            "[0.002 0.    1.    0.012 0.002 0.    0.    0.979 0.74  0.    0.706 0.\n",
            " 0.    0.    0.    0.001 1.    0.    0.009 1.    0.    0.65  0.    1.\n",
            " 0.094 0.998 1.    1.    0.    0.    0.    0.    0.    0.    0.    0.998\n",
            " 0.    0.    0.    0.    0.999 0.    0.    0.    0.    0.    0.281 0.909\n",
            " 0.    0.999]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X27Iy_4D62Dl",
        "outputId": "759225be-b4c5-429a-847a-24d067e51d6a"
      },
      "source": [
        "# Next we define the objective and check it gives the same output as sklearn.\n",
        "\n",
        "from sklearn.metrics import log_loss\n",
        "from jax.scipy.special import logsumexp\n",
        "#from scipy.misc import logsumexp\n",
        "\n",
        "def NLL_unstable(weights, batch):\n",
        "    inputs, targets = batch\n",
        "    p1 = predict_prob(weights, inputs)\n",
        "    logprobs = jnp.log(p1) * targets + jnp.log(1 - p1) * (1 - targets)\n",
        "    N = inputs.shape[0]\n",
        "    return -jnp.sum(logprobs)/N\n",
        "\n",
        "\n",
        "def NLL(weights, batch):\n",
        "    # Use log-sum-exp trick\n",
        "    inputs, targets = batch\n",
        "    # p1 = 1/(1+exp(-logit)), p0 = 1/(1+exp(+logit))\n",
        "    logits = predict_logit(weights, inputs).reshape((-1,1))\n",
        "    N = logits.shape[0]\n",
        "    logits_plus = jnp.hstack([jnp.zeros((N,1)), logits]) # e^0=1\n",
        "    logits_minus = jnp.hstack([jnp.zeros((N,1)), -logits])\n",
        "    logp1 = -logsumexp(logits_minus, axis=1)\n",
        "    logp0 = -logsumexp(logits_plus, axis=1)\n",
        "    logprobs = logp1 * targets + logp0 * (1-targets)\n",
        "    return -jnp.sum(logprobs)/N\n",
        "\n",
        "# We can use a small amount of L2 regularization, for numerical stability\n",
        "def PNLL(weights, batch, l2_penalty=1e-5):\n",
        "    nll = NLL(weights, batch)\n",
        "    l2_norm = jnp.sum(jnp.power(weights, 2)) # squared L2 norm\n",
        "    return nll + l2_penalty*l2_norm\n",
        "\n",
        "# We evaluate the training loss at the MLE, where the parameter values are \"extreme\".\n",
        "nll_train = log_loss(y_train, predict_prob(w_mle_sklearn, X_train))\n",
        "nll_train2 = NLL(w_mle_sklearn, (X_train, y_train))\n",
        "nll_train3 = NLL_unstable(w_mle_sklearn, (X_train, y_train))\n",
        "print(nll_train)\n",
        "print(nll_train2)\n",
        "print(nll_train3)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.06907700925379459\n",
            "0.06907699\n",
            "nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8PxEPaC7pMK"
      },
      "source": [
        "# Next we check the gradients compared to the manual formulas.\n",
        "# For simplicity, we initially just do this for a single random example.\n",
        "\n",
        "np.random.seed(42)\n",
        "D = 5\n",
        "w = np.random.randn(D)\n",
        "x = np.random.randn(D)\n",
        "y = 0 \n",
        "\n",
        "#d/da sigmoid(a) = s(a) * (1-s(a))\n",
        "deriv_sigmoid = lambda a: sigmoid(a) * (1-sigmoid(a))\n",
        "deriv_sigmoid_jax = grad(sigmoid)\n",
        "a = 1.5 # a random logit\n",
        "assert jnp.isclose(deriv_sigmoid(a), deriv_sigmoid_jax(a))\n",
        "\n",
        "# mu(w)=sigmoid(w'x), d/dw mu(w) = mu * (1-mu) .* x\n",
        "def mu(w): return sigmoid(jnp.dot(w,x))\n",
        "def deriv_mu(w): return mu(w) * (1-mu(w)) * x\n",
        "deriv_mu_jax =  grad(mu)\n",
        "assert jnp.allclose(deriv_mu(w), deriv_mu_jax(w))\n",
        "\n",
        "# NLL(w) = -[y*log(mu) + (1-y)*log(1-mu)]\n",
        "# d/dw NLL(w) = (mu-y)*x\n",
        "def nll(w): return -(y*jnp.log(mu(w)) + (1-y)*jnp.log(1-mu(w)))\n",
        "def deriv_nll(w): return (mu(w)-y)*x\n",
        "deriv_nll_jax = grad(nll)\n",
        "assert jnp.allclose(deriv_nll(w), deriv_nll_jax(w))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVT2tlmA7z72",
        "outputId": "e9fa28ab-ca6c-423f-ebb3-0c49a2a12a5d"
      },
      "source": [
        "# Now let's check the gradients on the batch version of our data.\n",
        "\n",
        "N = X_train.shape[0]\n",
        "mu = predict_prob(w_mle_sklearn, X_train)\n",
        "\n",
        "g1 = grad(NLL)(w_mle_sklearn, (X_train, y_train))\n",
        "g2 = jnp.sum(jnp.dot(jnp.diag(mu - y_train), X_train), axis=0)/N\n",
        "print(g1)\n",
        "print(g2)\n",
        "assert jnp.allclose(g1, g2, atol=1e-2)\n",
        "\n",
        "H1 = hessian(NLL)(w_mle_sklearn, (X_train, y_train))\n",
        "S = jnp.diag(mu * (1-mu))\n",
        "H2 = jnp.dot(jnp.dot(X_train.T, S), X_train)/N\n",
        "print(H1)\n",
        "print(H2)\n",
        "assert jnp.allclose(H1, H2, atol=1e-2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 3.5801623e-08  7.0655005e-07 -9.9190243e-07 -1.4292980e-06]\n",
            "[ 2.3841858e-08  6.9618227e-07 -1.0067224e-06 -1.4327467e-06]\n",
            "[[0.80245787 0.36579472 0.6444712  0.2132109 ]\n",
            " [0.36579472 0.1684845  0.29427886 0.09809215]\n",
            " [0.64447117 0.29427886 0.5187084  0.17146072]\n",
            " [0.21321094 0.09809215 0.17146073 0.05745751]]\n",
            "[[0.80245805 0.36579484 0.6444712  0.21321094]\n",
            " [0.36579484 0.1684845  0.29427883 0.09809214]\n",
            " [0.6444711  0.29427883 0.51870865 0.17146075]\n",
            " [0.21321093 0.09809214 0.17146075 0.05745751]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_LREVYj79IW",
        "outputId": "3125bf39-8feb-48a8-e7a9-5ed9c8c626d5"
      },
      "source": [
        "# Finally, use BFGS batch optimizer to compute MLE, and compare to sklearn\n",
        "\n",
        "import scipy.optimize\n",
        "\n",
        "def training_loss(w):\n",
        "    return NLL(w, (X_train, y_train))\n",
        "\n",
        "def training_grad(w):\n",
        "    return grad(training_loss)(w)\n",
        "\n",
        "np.random.seed(43)\n",
        "N, D = X_train.shape \n",
        "w_init = np.random.randn(D)\n",
        "w_mle_scipy = scipy.optimize.minimize(training_loss, w_init, jac=training_grad, method='BFGS').x\n",
        "\n",
        "\n",
        "print(\"parameters from sklearn {}\".format(w_mle_sklearn))\n",
        "print(\"parameters from scipy-bfgs {}\".format(w_mle_scipy))\n",
        "assert jnp.allclose(w_mle_sklearn, w_mle_scipy, atol=1e-1)\n",
        "\n",
        "prob_scipy = predict_prob(w_mle_scipy, X_test)\n",
        "prob_sklearn = predict_prob(w_mle_sklearn, X_test)\n",
        "print(jnp.round(prob_scipy, 3))\n",
        "print(jnp.round(prob_sklearn, 3))\n",
        "\n",
        "assert jnp.allclose(prob_scipy, prob_sklearn, atol=1e-2)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "parameters from sklearn [-4.41378437 -9.11061763  6.53872233 12.68572678]\n",
            "parameters from scipy-bfgs [-4.43822388 -9.04306242  6.52521732 12.7028332 ]\n",
            "[0.002 0.    1.    0.012 0.002 0.    0.    0.979 0.732 0.    0.711 0.\n",
            " 0.    0.    0.    0.001 1.    0.    0.009 1.    0.    0.654 0.    1.\n",
            " 0.095 0.998 1.    1.    0.    0.    0.    0.    0.    0.    0.    0.998\n",
            " 0.    0.    0.    0.    0.999 0.    0.    0.    0.    0.    0.279 0.91\n",
            " 0.    0.999]\n",
            "[0.002 0.    1.    0.012 0.002 0.    0.    0.979 0.74  0.    0.706 0.\n",
            " 0.    0.    0.    0.001 1.    0.    0.009 1.    0.    0.65  0.    1.\n",
            " 0.094 0.998 1.    1.    0.    0.    0.    0.    0.    0.    0.    0.998\n",
            " 0.    0.    0.    0.    0.999 0.    0.    0.    0.    0.    0.281 0.909\n",
            " 0.    0.999]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGXLRuHI0Xrw"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    }
  ]
}