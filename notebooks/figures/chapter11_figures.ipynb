{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "# Use of this source code is governed by an MIT-style\n",
    "# license that can be found in the LICENSE file or at\n",
    "# https://opensource.org/licenses/MIT.\n",
    "\n",
    "# Author(s): Kevin P. Murphy (murphyk@gmail.com) and Mahmoud Soliman (mjs@aucegypt.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://opensource.org/licenses/MIT\" target=\"_parent\"><img src=\"https://img.shields.io/github/license/probml/pyprobml\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/probml/pyprobml/blob/master/notebooks/figures//chapter11_figures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloning the pyprobml repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/probml/pyprobml \n",
    "%cd pyprobml/scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing required software (This may take few minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install octave  -qq > /dev/null\n",
    "!apt-get install liboctave-dev -qq > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "\n",
    "DISCLAIMER = 'WARNING : Editing in VM - changes lost after reboot!!'\n",
    "from google.colab import files\n",
    "\n",
    "def interactive_script(script, i=True):\n",
    "  if i:\n",
    "    s = open(script).read()\n",
    "    if not s.split('\\n', 1)[0]==\"## \"+DISCLAIMER:\n",
    "      open(script, 'w').write(\n",
    "          f'## {DISCLAIMER}\\n' + '#' * (len(DISCLAIMER) + 3) + '\\n\\n' + s)\n",
    "    files.view(script)\n",
    "    %run $script\n",
    "  else:\n",
    "      %run $script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 11.1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Polynomial of degrees 1 and 2 fit to 21 datapoints.  \n",
    "Figure(s) generated by [linreg_poly_vs_degree.py](https://github.com/probml/pyprobml/blob/master/scripts/linreg_poly_vs_degree.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_script(\"linreg_poly_vs_degree.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 11.2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a) Contours of the RSS error surface for the example in \\cref  fig:linregPolyDegree1 . The blue cross represents the MLE. (b) Corresponding surface plot.  \n",
    "Figure(s) generated by [linreg_contours_sse_plot.py](https://github.com/probml/pyprobml/blob/master/scripts/linreg_contours_sse_plot.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_script(\"linreg_contours_sse_plot.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 11.3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Graphical interpretation of least squares for $m=3$ equations and $n=2$ unknowns when solving the system $\\mathbf  A \\mathbf  x = \\mathbf  b $. $\\mathbf  a _1$ and $\\mathbf  a _2$ are the columns of $\\mathbf  A $, which define a 2d linear subspace embedded in $\\mathbb  R ^3$. The target vector $\\mathbf  b $ is a vector in $\\mathbb  R ^3$; its orthogonal projection onto the linear subspace is denoted $ \\mathbf  b  $. The line from $\\mathbf  b $ to $ \\mathbf  b  $ is the vector of residual errors, whose norm we want to minimize. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 11.4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Regression coefficients over time for the 1d model in \\cref  fig:linregPoly2 (a).  \n",
    "Figure(s) generated by [linregOnlineDemo.m](https://github.com/probml/pmtk3/blob/master/demos/linregOnlineDemo.m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W linregOnlineDemo.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 11.5:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Residual plot for polynomial regression of degree 1 and 2 for the functions in \\cref  fig:linregPoly2 (a-b).  \n",
    "Figure(s) generated by [linreg_poly_vs_degree.py](https://github.com/probml/pyprobml/blob/master/scripts/linreg_poly_vs_degree.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_script(\"linreg_poly_vs_degree.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 11.6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Fit vs actual plots for polynomial regression of degree 1 and 2 for the functions in \\cref  fig:linregPoly2 (a-b).  \n",
    "Figure(s) generated by [linreg_poly_vs_degree.py](https://github.com/probml/pyprobml/blob/master/scripts/linreg_poly_vs_degree.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_script(\"linreg_poly_vs_degree.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 11.7:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a-c) Ridge regression applied to a degree 14 polynomial fit to 21 datapoints. (d) MSE vs strength of regularizer. The degree of regularization increases from left to right, so model complexity decreases from left to right.  \n",
    "Figure(s) generated by [linreg_poly_ridge.py](https://github.com/probml/pyprobml/blob/master/scripts/linreg_poly_ridge.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_script(\"linreg_poly_ridge.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 11.8:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Geometry of ridge regression. The likelihood is shown as an ellipse, and the prior is shown as a circle centered on the origin. Adapted from Figure 3.15 of <a href='#BishopBook'>[Bis06]</a> .  \n",
    "Figure(s) generated by [geom_ridge.py](https://github.com/probml/pyprobml/blob/master/scripts/geom_ridge.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_script(\"geom_ridge.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 11.9:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a) Illustration of robust linear regression.  \n",
    "Figure(s) generated by [linregRobustDemoCombined.m](https://github.com/probml/pmtk3/blob/master/demos/linregRobustDemoCombined.m) [huberLossPlot.m](https://github.com/probml/pmtk3/blob/master/demos/huberLossPlot.m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W linregRobustDemoCombined.m >> _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W huberLossPlot.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 11.10:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Illustration of $\\ell _1$ (left) vs $\\ell _2$ (right) regularization of a least squares problem. Adapted from Figure 3.12 of <a href='#Hastie01'>[HTF01]</a> . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 11.11:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Left: soft thresholding. Right: hard thresholding. In both cases, the horizontal axis is the residual error incurred by making predictions using all the coefficients except for $w_k$, and the vertical axis is the estimated coefficient $ w _k$ that minimizes this penalized residual. The flat region in the middle is the interval $[-\\lambda ,+\\lambda ]$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 11.12:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a) Profiles of ridge coefficients for the prostate cancer example vs bound $B$ on $\\ell _2$ norm of $\\mathbf  w $, so small $B$ (large $\\lambda $) is on the left. The vertical line is the value chosen by 5-fold CV using the 1 standard error rule. Adapted from Figure 3.8 of <a href='#HastieBook'>[HTF09]</a> .  \n",
    "Figure(s) generated by [ridgePathProstate.m](https://github.com/probml/pmtk3/blob/master/demos/ridgePathProstate.m) [lassoPathProstate.m](https://github.com/probml/pmtk3/blob/master/demos/lassoPathProstate.m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W ridgePathProstate.m >> _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W lassoPathProstate.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 11.13:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Values of the coefficients for linear regression model fit to prostate cancer dataset as we vary the strength of the $\\ell _1$ regularizer. These numbers are plotted in \\cref  fig:lassoPathProstate (b). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 11.14:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a) Boxplot displaying (absolute value of) prediction errors on the prostate cancer test set for different regression methods.  \n",
    "Figure(s) generated by [prostateComparison.py](https://github.com/probml/pyprobml/blob/master/scripts/prostateComparison.py) [sparseSensingDemo.m](https://github.com/probml/pmtk3/blob/master/demos/sparseSensingDemo.m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_script(\"prostateComparison.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W sparseSensingDemo.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 11.15:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Illustration of group lasso where the original signal is piecewise Gaussian. (a) Original signal. (b) Vanilla lasso estimate. (c) Group lasso estimate using a $\\ell _2$ norm on the blocks. (d) Group lasso estimate using an $\\ell _ \\infty  $ norm on the blocks. Adapted from Figures 3-4 of <a href='#Wright09'>[WNF09]</a> .  \n",
    "Figure(s) generated by [groupLassoDemo.m](https://github.com/probml/pmtk3/blob/master/demos/groupLassoDemo.m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W groupLassoDemo.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 11.16:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Same as \\cref  fig:groupLassoGauss , except the original signal is piecewise constant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 11.17:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Sequential Bayesian inference of the parameters of a linear regression model $p(y|\\mathbf  x ) = \\mathcal  N (y | w_0 + w_1 x_1, \\sigma ^2)$. Left column: likelihood function for current data point. Middle column: posterior given first $N$ data points, $p(w_0,w_1|\\mathbf  x _ 1:N ,y_ 1:N ,\\sigma ^2)$. Right column: samples from the current posterior predictive distribution. Row 1: prior distribution ($N=0$). Row 2: after 1 data point. Row 3: after 2 data points. Row 4: after 100 data points. The white cross in columns 1 and 2 represents the true parameter value; we see that the mode of the posterior rapidly converges to this point. The blue circles in column 3 are the observed data points. Adapted from Figure 3.7 of <a href='#BishopBook'>[Bis06]</a> .  \n",
    "Figure(s) generated by [linreg_2d_bayes_demo.py](https://github.com/probml/pyprobml/blob/master/scripts/linreg_2d_bayes_demo.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_script(\"linreg_2d_bayes_demo.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 11.18:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Posterior samples of $p(w_0,w_1| \\mathcal  D  )$ for 1d linear regression model $p(y|x,\\boldsymbol  \\theta  )=\\mathcal  N (y|w_0 + w_1 x, \\sigma ^2)$ with a Gaussian prior. (a) Original data. (b) Centered data.  \n",
    "Figure(s) generated by [linreg_2d_bayes_centering_pymc3.py](https://github.com/probml/pyprobml/blob/master/scripts/linreg_2d_bayes_centering_pymc3.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_script(\"linreg_2d_bayes_centering_pymc3.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 11.19:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a) Plugin approximation to predictive density (we plug in the MLE of the parameters) when fitting a second degree polynomial to some 1d data. (b) Posterior predictive density, obtained by integrating out the parameters. Black curve is posterior mean, error bars are 2 standard deviations of the posterior predictive density. (c) 10 samples from the plugin approximation to posterior predictive distribution. (d) 10 samples from the true posterior predictive distribution.  \n",
    "Figure(s) generated by [linreg_post_pred_plot.py](https://github.com/probml/pyprobml/blob/master/scripts/linreg_post_pred_plot.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_script(\"linreg_post_pred_plot.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 11.20:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a) Representing lasso using a Gaussian scale mixture prior. (b) Graphical model for group lasso with 2 groups, the first has size $G_1=2$, the second has size $G_2=3$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 11.21:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  A hierarchical Bayesian linear regression model for the radon problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 11.22:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Posterior marginals for $\\alpha _c$ and $\\beta _c$ for each county in the radon model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 11.23:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Predictions from the radon model for 3 different counties in Minnesota. Black dots are observed datapoints. Green represents results of hierarchical (shared) prior, blue represents results of non-hierarchical prior. Thick lines are the result of using the posterior mean, thin lines are the result of using posterior samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 11.24:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a) Bivariate posterior $p(\\beta _c,\\sigma _ \\beta  | \\mathcal  D  )$ for the hierarchical radon model for county $c=75$ using centered parameterization. (b) Similar to (a) except we plot $p(\\cc@accent  \"707E  \\beta  _c,\\sigma _ \\beta  | \\mathcal  D  )$ for the non-centered parameterization. From   https://twiecki.io/blog/2017/02/08/bayesian-hierchical-non-centered/ . Used with kind permission of Thomas Wiecki. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 11.25:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Illustration of why ARD results in sparsity. The vector of inputs $\\mathbf  x $ does not point towards the vector of outputs $\\mathbf  y $, so the feature should be removed. (a) For finite $\\alpha $, the probability density is spread in directions away from $\\mathbf  y $. (b) When $\\alpha =\\infty $, the probability density at $\\mathbf  y $ is maximized. Adapted from Figure 8 of <a href='#Tipping01'>[Tip01]</a> . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 11.26:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a) A dynamic generalization of linear regression. (b) Illustration of the recursive least squares algorithm applied to the model $p(y|\\mathbf  x ,\\boldsymbol  \\theta  ) = \\mathcal  N (y|w_0 + w_1 x, \\sigma ^2)$. We plot the marginal posterior of $w_0$ and $w_1$ vs number of data points. (Error bars represent $\\mathbb  E \\left [ w_j|y_ 1:t  \\right ] \\pm \\sqrt  \\mathbb  V \\left [  w_j|y_ 1:t  \\right ] $.) After seeing all the data, we converge to the offline ML (least squares) solution, represented by the horizontal lines.  \n",
    "Figure(s) generated by [linregOnlineDemoKalman.m](https://github.com/probml/pmtk3/blob/master/demos/linregOnlineDemoKalman.m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W linregOnlineDemoKalman.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    " <a name='BishopBook'>[Bis06]</a> C. Bishop \"Pattern recognition and machine learning\". (2006). \n",
    "\n",
    "<a name='Hastie01'>[HTF01]</a> T. Hastie, R. Tibshirani and J. Friedman. \"The Elements of Statistical Learning\". (2001). \n",
    "\n",
    "<a name='HastieBook'>[HTF09]</a> T. Hastie, R. Tibshirani and J. Friedman. \"The Elements of Statistical Learning\". (2009). \n",
    "\n",
    "<a name='Tipping01'>[Tip01]</a> M. Tipping \"Sparse Bayesian learning and the relevance vector machine\". In: jmlr (2001). \n",
    "\n",
    "<a name='Wright09'>[WNF09]</a> S. Wright, R. Nowak and M. Figueiredo. \"Sparse reconstruction by separable approximation\". In: IEEE Trans. on Signal Processing (2009). \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
