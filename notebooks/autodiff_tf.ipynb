{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "autodiff_tf.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/probml/pyprobml/blob/master/book1/supplements/autodiff_tf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xX4GSX3Fwt1S"
      },
      "source": [
        "# Automatic differentiation in tensorflow 2\n",
        "\n",
        "We use binary logistic regression as a running example.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7H7qrB8xT8J"
      },
      "source": [
        "# Standard Python libraries\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "import imageio\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "import sklearn\n",
        "\n",
        "import seaborn as sns;\n",
        "sns.set(style=\"ticks\", color_codes=True)\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option('precision', 2) # 2 decimal places\n",
        "pd.set_option('display.max_rows', 20)\n",
        "pd.set_option('display.max_columns', 30)\n",
        "pd.set_option('display.width', 100) # wide windows\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TdbcDI_XXhS"
      },
      "source": [
        "\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "    IS_COLAB = True\n",
        "except Exception:\n",
        "    IS_COLAB = False\n",
        "\n",
        "# TensorFlow â‰¥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "print(\"tf version {}\".format(tf.__version__))\n",
        "\n",
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"No GPU was detected. DNNs can be very slow without a GPU.\")\n",
        "    if IS_COLAB:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "az0915TG19Sl",
        "outputId": "520c6034-6f11-4bc9-cd5a-3f3a024e8c25"
      },
      "source": [
        "## Compute gradient of loss \"by hand\" using numpy\n",
        "\n",
        "from scipy.special import logsumexp\n",
        "\n",
        "def BCE_with_logits(logits, targets):\n",
        "    N = logits.shape[0]\n",
        "    logits = logits.reshape(N,1)\n",
        "    logits_plus = np.hstack([np.zeros((N,1)), logits]) # e^0=1\n",
        "    logits_minus = np.hstack([np.zeros((N,1)), -logits])\n",
        "    logp1 = -logsumexp(logits_minus, axis=1)\n",
        "    logp0 = -logsumexp(logits_plus, axis=1)\n",
        "    logprobs = logp1 * targets + logp0 * (1-targets)\n",
        "    return -np.sum(logprobs)/N\n",
        "\n",
        "def sigmoid(x): return 0.5 * (np.tanh(x / 2.) + 1)\n",
        "\n",
        "def predict_logit(weights, inputs):\n",
        "    return np.dot(inputs, weights) # Already vectorized\n",
        "\n",
        "def predict_prob(weights, inputs):\n",
        "    return sigmoid(predict_logit(weights, inputs))\n",
        "\n",
        "def NLL(weights, batch):\n",
        "    X, y = batch\n",
        "    logits = predict_logit(weights, X)\n",
        "    return BCE_with_logits(logits, y)\n",
        "\n",
        "def NLL_grad(weights, batch):\n",
        "    X, y = batch\n",
        "    N = X.shape[0]\n",
        "    mu = predict_prob(weights, X)\n",
        "    g = np.sum(np.dot(np.diag(mu - y), X), axis=0)/N\n",
        "    return g\n",
        "\n",
        "np.random.seed(0)\n",
        "N = 100\n",
        "D = 5\n",
        "X = np.random.randn(N, D)\n",
        "w = 10*np.random.randn(D)\n",
        "mu = predict_prob(w, X)\n",
        "y = np.random.binomial(n=1, p=mu, size=N)\n",
        "\n",
        "X_test = X\n",
        "y_test = y\n",
        "\n",
        "y_pred = predict_prob(w, X_test)\n",
        "loss = NLL(w, (X_test, y_test))\n",
        "grad_np = NLL_grad(w, (X_test, y_test))\n",
        "print(\"params {}\".format(w))\n",
        "#print(\"pred {}\".format(y_pred))\n",
        "print(\"loss {}\".format(loss))\n",
        "print(\"grad {}\".format(grad_np))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "params [ 3.8273243  -0.34242281 10.96346846 -2.34215801 -3.47450652]\n",
            "loss 0.05501843790657687\n",
            "grad [-0.01360904  0.00325892  0.00844617  0.00848175  0.01390088]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ekyo0EEO2KL7",
        "outputId": "1ac5fdb2-1307-4650-ed36-a9a8744f2878"
      },
      "source": [
        "\n",
        "w_tf = tf.Variable(np.reshape(w, (D,1)))  \n",
        "x_test_tf = tf.convert_to_tensor(X_test, dtype=np.float64) \n",
        "y_test_tf = tf.convert_to_tensor(np.reshape(y_test, (-1,1)), dtype=np.float64)\n",
        "with tf.GradientTape() as tape:\n",
        "    logits = tf.linalg.matmul(x_test_tf, w_tf)\n",
        "    y_pred = tf.math.sigmoid(logits)\n",
        "    loss_batch = tf.nn.sigmoid_cross_entropy_with_logits(y_test_tf, logits)\n",
        "    loss_tf = tf.reduce_mean(loss_batch, axis=0)\n",
        "grad_tf = tape.gradient(loss_tf, [w_tf])\n",
        "grad_tf = grad_tf[0][:,0].numpy()\n",
        "assert np.allclose(grad_np, grad_tf)\n",
        "\n",
        "print(\"params {}\".format(w_tf))\n",
        "#print(\"pred {}\".format(y_pred))\n",
        "print(\"loss {}\".format(loss_tf))\n",
        "print(\"grad {}\".format(grad_tf))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "params <tf.Variable 'Variable:0' shape=(5, 1) dtype=float64, numpy=\n",
            "array([[ 3.8273243 ],\n",
            "       [-0.34242281],\n",
            "       [10.96346846],\n",
            "       [-2.34215801],\n",
            "       [-3.47450652]])>\n",
            "loss [0.05501844]\n",
            "grad [-0.01360904  0.00325892  0.00844617  0.00848175  0.01390088]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}