{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04eb6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Oct 27 10:08:08 2020\n",
    "\n",
    "@author: kpmurphy\n",
    "\"\"\"\n",
    "\n",
    "# Fit logistic regression models to 2d data using polynomial features\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    from sklearn.datasets import make_classification, make_blobs\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -qq scikit-learn\n",
    "    from sklearn.datasets import make_classification, make_blobs\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.colors as mcol\n",
    "import os\n",
    "\n",
    "try:\n",
    "    import probml_utils as pml\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -qq git+https://github.com/probml/probml-utils.git\n",
    "    import probml_utils as pml\n",
    "\n",
    "\n",
    "def plot_data(ax, X, y, is_train=True):\n",
    "    X0 = X[:, 0]\n",
    "    X1 = X[:, 1]\n",
    "    colors = [\"red\", \"blue\"]\n",
    "    if is_train:\n",
    "        markers = [\"x\", \"*\"]\n",
    "    else:\n",
    "        markers = [\"o\", \"s\"]\n",
    "    for x0, x1, cls in zip(X0, X1, y):\n",
    "        color = colors[int(cls) - 1]\n",
    "        marker = markers[int(cls) - 1]\n",
    "        ax.scatter(x0, x1, marker=marker, color=color)\n",
    "    ax.set_ylim(-2.75, 2.75)\n",
    "\n",
    "\n",
    "def plot_predictions(ax, xx, yy, transformer, model):\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    grid2 = transformer.transform(grid)[:, 1:]\n",
    "    Z = model.predict(grid2).reshape(xx.shape)\n",
    "    ax.pcolormesh(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.1)\n",
    "    # plt.axis('off')\n",
    "\n",
    "\n",
    "def make_data(ntrain, ntest):\n",
    "    n = ntrain + ntest\n",
    "    X, y = make_classification(\n",
    "        n_samples=n, n_features=2, n_redundant=0, n_classes=2, n_clusters_per_class=2, class_sep=0.1, random_state=1\n",
    "    )\n",
    "    X0, y0 = make_blobs(n_samples=[n, n], n_features=2, cluster_std=2, random_state=1)\n",
    "    Xtrain = X[:ntrain, :]\n",
    "    ytrain = y[:ntrain]\n",
    "    Xtest = X[ntrain:, :]\n",
    "    ytest = y[ntrain:]\n",
    "    xmin = np.min(X[:, 0])\n",
    "    xmax = np.max(X[:, 0])\n",
    "    ymin = np.min(X[:, 1])\n",
    "    ymax = np.max(X[:, 1])\n",
    "    xx, yy = np.meshgrid(np.linspace(xmin, xmax, n), np.linspace(ymin, ymax, 200))\n",
    "    return Xtrain, ytrain, Xtest, ytest, xx, yy\n",
    "\n",
    "\n",
    "ntrain = 50\n",
    "ntest = 1000\n",
    "Xtrain, ytrain, Xtest, ytest, xx, yy = make_data(ntrain, ntest)\n",
    "\n",
    "\n",
    "### Try different strngth regularizers\n",
    "degree = 4\n",
    "# C =1/lambda, so large C is large variance is small regularization\n",
    "C_list = np.logspace(0, 5, 7)\n",
    "# C_list = [1, 10, 100, 200, 500, 1000]\n",
    "plot_list = C_list\n",
    "err_train_list = []\n",
    "err_test_list = []\n",
    "w_list = []\n",
    "for i, C in enumerate(C_list):\n",
    "    transformer = PolynomialFeatures(degree)\n",
    "    name = \"Reg{:d}-Degree{}\".format(int(C), degree)\n",
    "    XXtrain = transformer.fit_transform(Xtrain)[:, 1:]  # skip the first column of 1s\n",
    "    model = LogisticRegression(C=C)\n",
    "    model = model.fit(XXtrain, ytrain)\n",
    "    w = model.coef_[0]\n",
    "    w_list.append(w)\n",
    "    ytrain_pred = model.predict(XXtrain)\n",
    "    nerrors_train = np.sum(ytrain_pred != ytrain)\n",
    "    err_train_list.append(nerrors_train / ntrain)\n",
    "    XXtest = transformer.fit_transform(Xtest)[:, 1:]  # skip the first column of 1s\n",
    "    ytest_pred = model.predict(XXtest)\n",
    "    nerrors_test = np.sum(ytest_pred != ytest)\n",
    "    err_test_list.append(nerrors_test / ntest)\n",
    "\n",
    "    if C in plot_list:\n",
    "        fig, ax = plt.subplots()\n",
    "        plot_predictions(ax, xx, yy, transformer, model)\n",
    "        plot_data(ax, Xtrain, ytrain, is_train=True)\n",
    "        # plot_data(ax, Xtest, ytest, is_train=False)\n",
    "        ax.set_title(name)\n",
    "        fname = \"logreg_poly_surface-{}.png\".format(name)\n",
    "        pml.savefig(fname)\n",
    "        plt.draw()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(C_list, err_train_list, \"x-\", label=\"train\")\n",
    "plt.plot(C_list, err_test_list, \"o-\", label=\"test\")\n",
    "plt.legend()\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Inverse regularization\")\n",
    "plt.ylabel(\"error rate\")\n",
    "pml.savefig(\"logreg_poly_vs_reg-Degree{}.pdf\".format(degree))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
